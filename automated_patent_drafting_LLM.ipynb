{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sJ-fzarbjivh"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BX-ICj2fjivj",
        "outputId": "78d8e74c-c6ed-436b-e4b8-41fa27956222"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.4.1+cu121\n",
            "4.44.2\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import transformers\n",
        "\n",
        "print(torch.__version__)\n",
        "print(transformers.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install openai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y0uUXwFNms-G",
        "outputId": "541abc50-fccc-4788-f051-53cc3bab2f21"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openai\n",
            "  Downloading openai-1.47.1-py3-none-any.whl.metadata (24 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Collecting httpx<1,>=0.23.0 (from openai)\n",
            "  Downloading httpx-0.27.2-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting jiter<1,>=0.4.0 (from openai)\n",
            "  Downloading jiter-0.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (2.9.2)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.5)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.10/dist-packages (from openai) (4.12.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2024.8.30)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai)\n",
            "  Downloading httpcore-1.0.5-py3-none-any.whl.metadata (20 kB)\n",
            "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (2.23.4)\n",
            "Downloading openai-1.47.1-py3-none-any.whl (375 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m375.6/375.6 kB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpx-0.27.2-py3-none-any.whl (76 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jiter-0.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (318 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m318.9/318.9 kB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: jiter, h11, httpcore, httpx, openai\n",
            "Successfully installed h11-0.14.0 httpcore-1.0.5 httpx-0.27.2 jiter-0.5.0 openai-1.47.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install google-cloud-bigquery"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G3TDUfVsoX8O",
        "outputId": "b8406b34-4659-46ab-d6fc-1fa839612bce"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: google-cloud-bigquery in /usr/local/lib/python3.10/dist-packages (3.25.0)\n",
            "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-cloud-bigquery) (2.19.2)\n",
            "Requirement already satisfied: google-auth<3.0.0dev,>=2.14.1 in /usr/local/lib/python3.10/dist-packages (from google-cloud-bigquery) (2.27.0)\n",
            "Requirement already satisfied: google-cloud-core<3.0.0dev,>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-bigquery) (2.4.1)\n",
            "Requirement already satisfied: google-resumable-media<3.0dev,>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-bigquery) (2.7.2)\n",
            "Requirement already satisfied: packaging>=20.0.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-bigquery) (24.1)\n",
            "Requirement already satisfied: python-dateutil<3.0dev,>=2.7.2 in /usr/local/lib/python3.10/dist-packages (from google-cloud-bigquery) (2.8.2)\n",
            "Requirement already satisfied: requests<3.0.0dev,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-bigquery) (2.32.3)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-cloud-bigquery) (1.65.0)\n",
            "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0.dev0,>=3.19.5 in /usr/local/lib/python3.10/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-cloud-bigquery) (3.20.3)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.10/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-cloud-bigquery) (1.24.0)\n",
            "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-cloud-bigquery) (1.64.1)\n",
            "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-cloud-bigquery) (1.48.2)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0.0dev,>=2.14.1->google-cloud-bigquery) (5.5.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0.0dev,>=2.14.1->google-cloud-bigquery) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0.0dev,>=2.14.1->google-cloud-bigquery) (4.9)\n",
            "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /usr/local/lib/python3.10/dist-packages (from google-resumable-media<3.0dev,>=0.6.0->google-cloud-bigquery) (1.6.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil<3.0dev,>=2.7.2->google-cloud-bigquery) (1.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0dev,>=2.21.0->google-cloud-bigquery) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0dev,>=2.21.0->google-cloud-bigquery) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0dev,>=2.21.0->google-cloud-bigquery) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0dev,>=2.21.0->google-cloud-bigquery) (2024.8.30)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.0dev,>=2.14.1->google-cloud-bigquery) (0.6.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "os.environ['GOOGLE_APPLICATION_CREDENTIALS']='/content/drive/MyDrive/Colab Notebooks/haystack-436323-fd77f9e327cc.json'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TZs5c1gtqjbi",
        "outputId": "e4f5846a-b1e4-4bec-d369-ec6abf51ded7"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import openai\n",
        "from transformers import pipeline\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from google.colab import userdata\n",
        "import time\n",
        "\n",
        "# Set OpenAI API key for GPT-4\n",
        "openai.api_key = userdata.get('OPENAI_API_KEY')\n",
        "os.environ['OPENAI_API_KEY'] = userdata.get('OPENAI_API_KEY')\n",
        "# Timer decorator to track execution time of functions\n",
        "def timer_decorator(func):\n",
        "    def wrapper(*args, **kwargs):\n",
        "        start_time = time.time()\n",
        "        result = func(*args, **kwargs)\n",
        "        end_time = time.time()\n",
        "        print(f\"Function '{func.__name__}' took {end_time - start_time:.4f} seconds\")\n",
        "        return result\n",
        "    return wrapper\n",
        "\n",
        "# 1. Search Patents from Different Data Sources using a Stronger GPT-4 Integration\n",
        "@timer_decorator\n",
        "def search_patents_gpt(query, source, api_key=None):\n",
        "    \"\"\"\n",
        "    Searches for patents from different sources based on a flag.\n",
        "    Uses GPT-4 to process results and enhance the search quality.\n",
        "    Possible sources: 'lens', 'uspto', 'google_patents', 'wipo'\n",
        "    \"\"\"\n",
        "    print(f\"Searching patents with query: {query} on source: {source}\")\n",
        "\n",
        "    if source == \"lens\":\n",
        "        patents = search_patents_lens(query, api_key)\n",
        "    elif source == \"uspto\":\n",
        "        patents = search_patents_uspto(query)\n",
        "    elif source == \"google_patents\":\n",
        "        patents = search_google_patents(query)\n",
        "    elif source == \"wipo\":\n",
        "        patents = search_patents_wipo(query, api_key)\n",
        "    else:\n",
        "        raise ValueError(\"Invalid source provided. Choose from: 'lens', 'uspto', 'google_patents', 'wipo'.\")\n",
        "\n",
        "    print(f\"Patents retrieved: {patents}\")\n",
        "\n",
        "    if isinstance(patents, list) and len(patents) > 0:\n",
        "        titles = [patent['title'] for patent in patents]  # Extracting titles from the list of dictionaries\n",
        "        print(f\"Patent titles: {titles}\")\n",
        "        gpt_response = refine_search_with_gpt(query, titles)\n",
        "        return gpt_response\n",
        "    else:\n",
        "        print(\"No prior art found for comparison.\")\n",
        "        return \"No prior art found for comparison.\"\n",
        "\n",
        "\n",
        "# 2. Function to Enhance Search Using GPT-4\n",
        "from openai import OpenAI\n",
        "client = OpenAI()\n",
        "@timer_decorator\n",
        "def refine_search_with_gpt(query, titles):\n",
        "    \"\"\"\n",
        "    Uses GPT-4 to refine and summarize the patent titles retrieved from various sources.\n",
        "    \"\"\"\n",
        "    print(f\"Refining search with GPT-4. Query: {query}, Titles: {titles}\")\n",
        "\n",
        "    try:\n",
        "        response = client.chat.completions.create(\n",
        "            model=\"gpt-4\",\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": \"You are a patent expert tasked with finding prior art for a new patent claim.\"},\n",
        "                {\"role\": \"user\", \"content\": f\"Patent claim: {query}\\nHere are some potential prior art titles: {titles}. Which ones are the most relevant, and why?\"}\n",
        "            ]\n",
        "        )\n",
        "        gpt_result = response.choices[0].message.content\n",
        "        #response.choices[0].message['content']\n",
        "        print(f\"GPT-4 response: {gpt_result}\")\n",
        "        return gpt_result\n",
        "    except Exception as e:\n",
        "        print(f\"Error during GPT-4 API call: {e}\")\n",
        "        return \"Error in GPT-4 processing\"\n",
        "\n",
        "# 3. Fetch Patent Data from Lens.org API (requires an API key)\n",
        "@timer_decorator\n",
        "def search_patents_lens(query, api_key):\n",
        "    \"\"\"\n",
        "    Searches Lens.org for patents based on a query.\n",
        "    Requires Lens.org API key for access.\n",
        "    \"\"\"\n",
        "    print(f\"Searching Lens.org with query: {query}\")\n",
        "\n",
        "    url = \"https://api.lens.org/scholarly/search\"\n",
        "    headers = {\n",
        "        'Authorization': f'Bearer {api_key}',\n",
        "        'Content-Type': 'application/json'\n",
        "    }\n",
        "    payload = {\n",
        "        \"query\": {\n",
        "            \"bool\": {\n",
        "                \"must\": [\n",
        "                    {\"match\": {\"text\": query}}\n",
        "                ]\n",
        "            }\n",
        "        },\n",
        "        \"size\": 10\n",
        "    }\n",
        "\n",
        "    response = requests.post(url, json=payload, headers=headers)\n",
        "\n",
        "    if response.status_code != 200:\n",
        "        print(f\"Error: Received status code {response.status_code}\")\n",
        "        print(f\"Response text: {response.text}\")\n",
        "        return {}\n",
        "\n",
        "    try:\n",
        "        return response.json()\n",
        "    except requests.exceptions.JSONDecodeError:\n",
        "        print(\"Error: Failed to parse JSON\")\n",
        "        print(f\"Response text: {response.text}\")\n",
        "        return {}\n",
        "\n",
        "# 4. Fetch Patent Data from USPTO (Public API)\n",
        "@timer_decorator\n",
        "def search_patents_uspto(query):\n",
        "    \"\"\"\n",
        "    Searches USPTO database for patents using bulk data search.\n",
        "    \"\"\"\n",
        "    print(f\"Searching USPTO with query: {query}\")\n",
        "\n",
        "    url = f\"https://developer.uspto.gov/ibd-api/v1/patent/application?searchText={query}&rows=10\"\n",
        "    response = requests.get(url)\n",
        "    return response.json()\n",
        "\n",
        "# 5. Fetch Patent Data from Google Patents (via BigQuery)\n",
        "@timer_decorator\n",
        "def search_google_patents(query):\n",
        "    \"\"\"\n",
        "    Searches Google Patents Public Dataset using BigQuery API.\n",
        "    Requires Google Cloud setup.\n",
        "    \"\"\"\n",
        "    print(f\"Searching Google Patents with query: {query}\")\n",
        "\n",
        "    client = bigquery.Client()\n",
        "\n",
        "    # Google Patents Public Datasets project and table\n",
        "    dataset_ref = \"patents-public-data.patents.publications\"\n",
        "\n",
        "    # Construct the query\n",
        "    sql = f\"\"\"\n",
        "        SELECT\n",
        "            publication_number,\n",
        "            title_localized,\n",
        "            ARRAY_TO_STRING(ARRAY_AGG(abstract.text), ' ') AS abstract_text,\n",
        "            filing_date\n",
        "        FROM `{dataset_ref}`,\n",
        "        UNNEST(abstract_localized) AS abstract\n",
        "        WHERE REGEXP_CONTAINS(abstract.text, r'{query}')\n",
        "        AND abstract.language = 'en'\n",
        "        GROUP BY publication_number, title_localized, filing_date\n",
        "        LIMIT 10\n",
        "    \"\"\"\n",
        "\n",
        "    # Execute the query\n",
        "    query_job = client.query(sql)\n",
        "\n",
        "    # Fetch the results\n",
        "    results = query_job.result()\n",
        "\n",
        "    # Convert results to a list of dictionaries\n",
        "    patents = []\n",
        "    for row in results:\n",
        "        patents.append({\n",
        "            \"publication_number\": row.publication_number,\n",
        "            \"title\": row.title_localized,\n",
        "            \"abstract\": row.abstract_text,\n",
        "            \"filing_date\": row.filing_date\n",
        "        })\n",
        "\n",
        "    print(f\"Google Patents Results: {patents}\")\n",
        "    return patents\n",
        "\n",
        "# 6. Fetch Patent Data from WIPO Patentscope API (requires an API key)\n",
        "@timer_decorator\n",
        "def search_patents_wipo(query, api_key):\n",
        "    \"\"\"\n",
        "    Searches WIPO Patentscope for patents using the API.\n",
        "    Requires WIPO API key.\n",
        "    \"\"\"\n",
        "    print(f\"Searching WIPO with query: {query}\")\n",
        "\n",
        "    url = f\"https://patentscope.wipo.int/search-api/rest/patents?query={query}&rows=10\"\n",
        "    headers = {\n",
        "        'Authorization': f'Bearer {api_key}'\n",
        "    }\n",
        "    response = requests.get(url, headers=headers)\n",
        "    return response.json()\n",
        "\n",
        "# 7. LLM for Semantic Similarity (Using Hugging Face Pipeline)\n",
        "@timer_decorator\n",
        "def check_novelty(new_patent_claim, prior_art_patents):\n",
        "    \"\"\"\n",
        "    Uses an LLM to check novelty by comparing new patent claims to prior art.\n",
        "    \"\"\"\n",
        "    print(f\"Checking novelty for patent claim: {new_patent_claim}\")\n",
        "    print(f\"Comparing with prior art patents: {prior_art_patents}\")\n",
        "\n",
        "    # Initialize HuggingFace pipeline for text similarity\n",
        "    similarity_model = pipeline('feature-extraction', model='sentence-transformers/all-MiniLM-L6-v2')\n",
        "\n",
        "    # Transform the new patent claim and the prior art patents\n",
        "    new_claim_embedding = similarity_model(new_patent_claim)[0]\n",
        "    prior_art_embeddings = [similarity_model(patent)[0] for patent in prior_art_patents]\n",
        "\n",
        "    # Compare similarity using cosine similarity\n",
        "    vectorizer = TfidfVectorizer()\n",
        "    corpus = [new_patent_claim] + prior_art_patents\n",
        "    tfidf_matrix = vectorizer.fit_transform(corpus)\n",
        "\n",
        "    # Compute cosine similarities\n",
        "    cosine_sim = cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:])\n",
        "\n",
        "    print(f\"Cosine Similarities: {cosine_sim}\")\n",
        "    return cosine_sim\n",
        "\n",
        "# 8. Full Pipeline for Patent Novelty Check using GPT-4 for stronger search\n",
        "@timer_decorator\n",
        "def patent_novelty_check(patent_claim, api_key, source):\n",
        "    \"\"\"\n",
        "    Pipeline to check the novelty of a patent claim against existing patents.\n",
        "    Uses GPT-4 to improve the search process.\n",
        "    \"\"\"\n",
        "    print(f\"Starting patent novelty check for: {patent_claim}, using source: {source}\")\n",
        "\n",
        "    prior_art = search_patents_gpt(patent_claim, source, api_key)\n",
        "\n",
        "    if prior_art and prior_art != \"No prior art found for comparison.\":\n",
        "        # Assuming that GPT-4 returns a string of relevant patents to compare\n",
        "        prior_art_list = prior_art.split('\\n')\n",
        "        print(f\"Prior art list from GPT-4: {prior_art_list}\")\n",
        "        similarities = check_novelty(patent_claim, prior_art_list)\n",
        "        print(\"Cosine Similarities to Prior Art: \", similarities)\n",
        "    else:\n",
        "        print(\"No relevant prior art found for comparison.\")\n",
        "    return prior_art_list, similarities\n",
        "api_key = \"your_lens_org_or_wipo_or_openai_api_key\"\n",
        "patent_claim = \"A method for enhancing machine learning model performance by using transfer learning techniques.\"\n",
        "patent_claim = 'machine learning'\n",
        "source = \"google_patents\"  # Choose from 'lens', 'uspto', 'google_patents', 'wipo'\n",
        "prior_art_list, similarities = patent_novelty_check(patent_claim, api_key, source)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "63U-28Cosb3k",
        "outputId": "c2712f0f-4d10-40e0-98b9-f09c12d04f50"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting patent novelty check for: machine learning, using source: google_patents\n",
            "Searching patents with query: machine learning on source: google_patents\n",
            "Searching Google Patents with query: machine learning\n",
            "Google Patents Results: [{'publication_number': 'US-2020320438-A1', 'title': [{'text': 'Prudent ensemble models in machine learning with high precision for use in network security', 'language': 'en', 'truncated': False}], 'abstract': 'Systems and methods include receiving a content item between a user device and a location on the Internet or an enterprise network; utilizing a trained machine learning ensemble model to determine whether the content item is malicious; responsive to the trained machine learning ensemble model determining the content item is malicious or determining the content item is benign but such determining is in a blind spot of the trained ensemble model, performing further processing on the content item; and, responsive to the trained machine learning ensemble model determining the content item is benign with such determination not in a blind spot of the trained machine learning ensemble model, allowing the content item. A blind spot is a location where the trained machine learning ensemble model has not seen any examples with a combination of features at the location or has examples with conflicting labels. Systems and methods include receiving a content item between a user device and a location on the Internet or an enterprise network; utilizing a trained machine learning ensemble model to determine whether the content item is malicious; responsive to the trained machine learning ensemble model determining the content item is malicious or determining the content item is benign but such determining is in a blind spot of the trained ensemble model, performing further processing on the content item; and, responsive to the trained machine learning ensemble model determining the content item is benign with such determination not in a blind spot of the trained machine learning ensemble model, allowing the content item. A blind spot is a location where the trained machine learning ensemble model has not seen any examples with a combination of features at the location or has examples with conflicting labels.', 'filing_date': 20190405}, {'publication_number': 'WO-2016208159-A1', 'title': [{'text': 'Information processing device, information processing system, information processing method, and storage medium', 'language': 'en', 'truncated': False}, {'text': 'Dispositif de traitement d&#39;informations, système de traitement d&#39;informations, procédé de traitement d&#39;informations et support de stockage', 'language': 'fr', 'truncated': False}, {'text': '情報処理装置、情報処理システム、情報処理方法、及び、記憶媒体', 'language': 'ja', 'truncated': False}], 'abstract': 'In order to create alert information that can be properly presented to an operator, an information processing device of the present invention includes: a dissimilarity calculating means for calculating dissimilarity between already received first alert information and newly received second alert information; a machine learning means for applying machine learning to the first alert information to generate a classifier which determines a classification result; and a determining means. The determining means applies the classifier to the second alert information to determine a classification result; sets a determined result for the classification result of the second alert information to set information which indicates there is no need of presentation for information which indicates the presentation of the second alert information, when the determined result is false detection which indicates erroneous detection and the dissimilarity is smaller than a prescribed threshold value; and sets information which indicates there is a need of the presentation for information which indicates the presentation of the second alert information, when the determined result is correct detection which indicates right detection, or when the determined result is false detection which indicates erroneous detection and the dissimilarity is equal to or greater than the prescribed threshold value. In order to create alert information that can be properly presented to an operator, an information processing device of the present invention includes: a dissimilarity calculating means for calculating dissimilarity between already received first alert information and newly received second alert information; a machine learning means for applying machine learning to the first alert information to generate a classifier which determines a classification result; and a determining means. The determining means applies the classifier to the second alert information to determine a classification result; sets a determined result for the classification result of the second alert information to set information which indicates there is no need of presentation for information which indicates the presentation of the second alert information, when the determined result is false detection which indicates erroneous detection and the dissimilarity is smaller than a prescribed threshold value; and sets information which indicates there is a need of the presentation for information which indicates the presentation of the second alert information, when the determined result is correct detection which indicates right detection, or when the determined result is false detection which indicates erroneous detection and the dissimilarity is equal to or greater than the prescribed threshold value.', 'filing_date': 20160615}, {'publication_number': 'US-11938957-B2', 'title': [{'text': 'Driving scenario sampling for training/tuning machine learning models for vehicles', 'language': 'en', 'truncated': False}], 'abstract': 'Enclosed are embodiments for sampling driving scenarios for training machine learning models. In an embodiment, a method comprises: assigning, using at least one processor, a set of initial physical states to a set of objects in a map for a set of simulated driving scenarios, wherein the set of initial physical states are assigned according to one or more outputs of a random number generator; generating, using the at least one processor, the set of simulated driving scenarios in the map using the initial physical states of the objects in the set of objects; selecting, using the at least one processor, samples of the simulated driving scenarios; training, using the at least one processor, a machine learning model using the selected samples; and operating, using a control circuit, a vehicle in an environment using the trained machine learning model. Enclosed are embodiments for sampling driving scenarios for training machine learning models. In an embodiment, a method comprises: assigning, using at least one processor, a set of initial physical states to a set of objects in a map for a set of simulated driving scenarios, wherein the set of initial physical states are assigned according to one or more outputs of a random number generator; generating, using the at least one processor, the set of simulated driving scenarios in the map using the initial physical states of the objects in the set of objects; selecting, using the at least one processor, samples of the simulated driving scenarios; training, using the at least one processor, a machine learning model using the selected samples; and operating, using a control circuit, a vehicle in an environment using the trained machine learning model.', 'filing_date': 20200824}, {'publication_number': 'US-12008503-B2', 'title': [{'text': 'Sensor risk assessment database', 'language': 'en', 'truncated': False}], 'abstract': 'An example operation may include one or more of receiving sensor data from one or more sensors associated with a building, storing a block including the sensor data to a shared ledger of a blockchain network, the one or more sensors associated with one or more buildings, requesting a risk assessment for the sensor data, by a blockchain node, calculating the risk assessment with one or more machine learning algorithms based on the sensor data, historical sensor blockchain data, importance of the one or more sensors, and a degree of concern related to the sensor data, providing the risk assessment to the blockchain node, and generating an alert in response to the risk assessment above a threshold. An example operation may include one or more of receiving sensor data from one or more sensors associated with a building, storing a block including the sensor data to a shared ledger of a blockchain network, the one or more sensors associated with one or more buildings, requesting a risk assessment for the sensor data, by a blockchain node, calculating the risk assessment with one or more machine learning algorithms based on the sensor data, historical sensor blockchain data, importance of the one or more sensors, and a degree of concern related to the sensor data, providing the risk assessment to the blockchain node, and generating an alert in response to the risk assessment above a threshold.', 'filing_date': 20181107}, {'publication_number': 'CN-109274314-B', 'title': [{'text': 'Machine learning device, servomotor control system, and machine learning method', 'language': 'en', 'truncated': False}, {'text': '机器学习装置、伺服电动机控制装置、伺服电动机控制系统以及机器学习方法', 'language': 'zh', 'truncated': False}], 'abstract': 'The invention provides a machine learning device, a servomotor control system, and a machine learning method. The machine learning device performs machine learning for a servo motor control device provided with a nonlinear friction compensator, and is provided with: a state information acquisition unit that acquires state information from the servo motor control device, the state information including a combination of a servo state including at least a positional deviation and a correction coefficient of the nonlinear friction compensation unit, by causing the servo motor control device to execute a predetermined program; a behavior information output unit that outputs behavior information including adjustment information of a combination of correction coefficients included in the state information to the servo motor control device; a return output unit that outputs a value of a return in reinforcement learning based on the positional deviation included in the status information; and a cost function updating unit for updating the behavior cost function based on the report value, the state information, and the behavior information output by the report output unit.', 'filing_date': 20180713}, {'publication_number': 'KR-102403174-B1', 'title': [{'text': '중요도에 따른 데이터 정제 방법 및 이를 실행시키기 위하여 기록매체에 기록된 컴퓨터 프로그램', 'language': 'ko', 'truncated': False}, {'text': 'Method for data purification according to importance, and computer program recorded on record-medium for executing method therefor', 'language': 'en', 'truncated': False}], 'abstract': 'The present invention proposes a method for cleaning data according to importance, which can clean 2D images pre-collected for artificial intelligence (AI) machine learning according to importance. The method comprises the steps in which: a learning data generating device evaluates importance by analyzing 2D images pre-collected for AI machine learning; and the learning data generating device cleans at least one of the pre-collected 2D images according to the evaluated importance. Therefore, the method can analyze the pre-collected 2D images to evaluate importance, and can clean at least one of the 2D images pre-collected according to the evaluated importance, thereby increasing the efficiency of machine learning as a 2D image with relatively low importance is cleaned.', 'filing_date': 20211221}, {'publication_number': 'US-11907882-B1', 'title': [{'text': 'Model validation of credit risk', 'language': 'en', 'truncated': False}], 'abstract': 'The innovation disclosed and claimed herein, in one aspect thereof, comprises systems and methods of validating models guided by machine learning algorithms. The innovation can begin by receiving a risk model for validation having multiple sets of data. A first data set is selected from as an input. Outputs are generated for validation. One output can be generating a second set of analysis results using a comparable algorithm to the risk model. Another output can be generating a second set of variables and transformations using a machine learning algorithm and an untransformed set of the selected variables to assess the set of selected transformations. Another output can be generating a third set of variables using one or more machine learning algorithms and an extended feature set of variables to assess the selected variables. The outputs are compared to the analysis results, coefficients, selected variables, and selected transformations. A report of the comparison is generated. The innovation disclosed and claimed herein, in one aspect thereof, comprises systems and methods of validating models guided by machine learning algorithms. The innovation can begin by receiving a risk model for validation having multiple sets of data. A first data set is selected from as an input. Outputs are generated for validation. One output can be generating a second set of analysis results using a comparable algorithm to the risk model. Another output can be generating a second set of variables and transformations using a machine learning algorithm and an untransformed set of the selected variables to assess the set of selected transformations. Another output can be generating a third set of variables using one or more machine learning algorithms and an extended feature set of variables to assess the selected variables. The outputs are compared to the analysis results, coefficients, selected variables, and selected transformations. A report of the comparison is generated.', 'filing_date': 20220825}, {'publication_number': 'CN-115643049-A', 'title': [{'text': 'Method for detecting mining action in real time based on encrypted flow analysis', 'language': 'en', 'truncated': False}, {'text': '一种基于加密流量分析的挖矿行为实时检测方法', 'language': 'zh', 'truncated': False}], 'abstract': 'The invention relates to a real-time detection method for mining excavation behaviors based on encrypted flow analysis, and belongs to the technical field of block chain encrypted network flow processing. The method extracts the length of the data packets, the number of the data packets, the arrival time interval of the data packets and the flow duration time of the data packets from the encrypted flow as characteristics, and further combines the characteristics with a machine learning model to realize accurate identification of the mining action. Under the scene that the network flow is encrypted, the method can detect the mining behavior of the encrypted currency, realize the timely blocking of the malicious mining flow and ensure that the equipment resources are prevented from being invaded. The invention only needs to be deployed at the gateway, reduces the implementation cost of a network administrator, and simultaneously only needs to passively monitor the flow without interfering the normal work of the network.', 'filing_date': 20220920}, {'publication_number': 'CN-117875443-A', 'title': [{'text': 'Implementing machine learning in a local APL edge device with power limitation', 'language': 'en', 'truncated': False}, {'text': '在具有功率限制的本地apl边缘设备中实现机器学习', 'language': 'zh', 'truncated': False}], 'abstract': 'A method performed by an Advanced Physical Layer (APL) -based edge device having power limitations is provided. The method includes applying an event driven framework that meets power constraints of an APL-based edge device to receive input data; the method includes applying an event driven framework to input data to invoke a Machine Learning (ML) model trained to analyze the input data and make inferences about one or more aspects of the industrial system based on the input data, and applying the invoked machine learning model to analyze the input data and make inferences about one or more aspects of the industrial system based on the input data. The APL based edge devices receive input data from one or more source field devices of the industrial system and/or use inferences to make decisions and apply actions to the industrial system.', 'filing_date': 20231010}, {'publication_number': 'CN-111444629-A', 'title': [{'text': 'Reinforcing steel bar corrosion parameter prediction method based on support vector machine', 'language': 'en', 'truncated': False}, {'text': '一种基于支持向量机的钢筋锈蚀参数预测方法', 'language': 'zh', 'truncated': False}], 'abstract': 'The invention relates to the technical fields of reinforced concrete, machine learning technology and big data, in particular to a method for predicting steel bar corrosion parameters based on a support vector machine. The steel bar corrosion parameter prediction method based on the support vector machine comprises the following steps: the method comprises the following steps: manufacturing a test piece of the corrosion reinforcing steel bar; step two: 3D scanning is carried out on the test piece, and a 3D image model is generated; step three: according to the 3D image model, calculating the specific characteristic parameters of the section of the test piece and the steel bar corrosion rate corresponding to the specific characteristic parameters of the section; step four: setting specific characteristic parameters of the cross section and the corrosion rate of the steel bar corresponding to the specific characteristic parameters as a group of basic data, inputting the multiple groups of basic data into a support vector machine for learning and training to obtain a prediction model for predicting the corrosion rate of the steel bar to be measured. The method can realize the prediction of the corrosion rate of the steel bar only by knowing the specific characteristic parameters of the corroded steel bar, and has high accuracy and convenient use.', 'filing_date': 20200415}]\n",
            "Function 'search_google_patents' took 1.3757 seconds\n",
            "Patents retrieved: [{'publication_number': 'US-2020320438-A1', 'title': [{'text': 'Prudent ensemble models in machine learning with high precision for use in network security', 'language': 'en', 'truncated': False}], 'abstract': 'Systems and methods include receiving a content item between a user device and a location on the Internet or an enterprise network; utilizing a trained machine learning ensemble model to determine whether the content item is malicious; responsive to the trained machine learning ensemble model determining the content item is malicious or determining the content item is benign but such determining is in a blind spot of the trained ensemble model, performing further processing on the content item; and, responsive to the trained machine learning ensemble model determining the content item is benign with such determination not in a blind spot of the trained machine learning ensemble model, allowing the content item. A blind spot is a location where the trained machine learning ensemble model has not seen any examples with a combination of features at the location or has examples with conflicting labels. Systems and methods include receiving a content item between a user device and a location on the Internet or an enterprise network; utilizing a trained machine learning ensemble model to determine whether the content item is malicious; responsive to the trained machine learning ensemble model determining the content item is malicious or determining the content item is benign but such determining is in a blind spot of the trained ensemble model, performing further processing on the content item; and, responsive to the trained machine learning ensemble model determining the content item is benign with such determination not in a blind spot of the trained machine learning ensemble model, allowing the content item. A blind spot is a location where the trained machine learning ensemble model has not seen any examples with a combination of features at the location or has examples with conflicting labels.', 'filing_date': 20190405}, {'publication_number': 'WO-2016208159-A1', 'title': [{'text': 'Information processing device, information processing system, information processing method, and storage medium', 'language': 'en', 'truncated': False}, {'text': 'Dispositif de traitement d&#39;informations, système de traitement d&#39;informations, procédé de traitement d&#39;informations et support de stockage', 'language': 'fr', 'truncated': False}, {'text': '情報処理装置、情報処理システム、情報処理方法、及び、記憶媒体', 'language': 'ja', 'truncated': False}], 'abstract': 'In order to create alert information that can be properly presented to an operator, an information processing device of the present invention includes: a dissimilarity calculating means for calculating dissimilarity between already received first alert information and newly received second alert information; a machine learning means for applying machine learning to the first alert information to generate a classifier which determines a classification result; and a determining means. The determining means applies the classifier to the second alert information to determine a classification result; sets a determined result for the classification result of the second alert information to set information which indicates there is no need of presentation for information which indicates the presentation of the second alert information, when the determined result is false detection which indicates erroneous detection and the dissimilarity is smaller than a prescribed threshold value; and sets information which indicates there is a need of the presentation for information which indicates the presentation of the second alert information, when the determined result is correct detection which indicates right detection, or when the determined result is false detection which indicates erroneous detection and the dissimilarity is equal to or greater than the prescribed threshold value. In order to create alert information that can be properly presented to an operator, an information processing device of the present invention includes: a dissimilarity calculating means for calculating dissimilarity between already received first alert information and newly received second alert information; a machine learning means for applying machine learning to the first alert information to generate a classifier which determines a classification result; and a determining means. The determining means applies the classifier to the second alert information to determine a classification result; sets a determined result for the classification result of the second alert information to set information which indicates there is no need of presentation for information which indicates the presentation of the second alert information, when the determined result is false detection which indicates erroneous detection and the dissimilarity is smaller than a prescribed threshold value; and sets information which indicates there is a need of the presentation for information which indicates the presentation of the second alert information, when the determined result is correct detection which indicates right detection, or when the determined result is false detection which indicates erroneous detection and the dissimilarity is equal to or greater than the prescribed threshold value.', 'filing_date': 20160615}, {'publication_number': 'US-11938957-B2', 'title': [{'text': 'Driving scenario sampling for training/tuning machine learning models for vehicles', 'language': 'en', 'truncated': False}], 'abstract': 'Enclosed are embodiments for sampling driving scenarios for training machine learning models. In an embodiment, a method comprises: assigning, using at least one processor, a set of initial physical states to a set of objects in a map for a set of simulated driving scenarios, wherein the set of initial physical states are assigned according to one or more outputs of a random number generator; generating, using the at least one processor, the set of simulated driving scenarios in the map using the initial physical states of the objects in the set of objects; selecting, using the at least one processor, samples of the simulated driving scenarios; training, using the at least one processor, a machine learning model using the selected samples; and operating, using a control circuit, a vehicle in an environment using the trained machine learning model. Enclosed are embodiments for sampling driving scenarios for training machine learning models. In an embodiment, a method comprises: assigning, using at least one processor, a set of initial physical states to a set of objects in a map for a set of simulated driving scenarios, wherein the set of initial physical states are assigned according to one or more outputs of a random number generator; generating, using the at least one processor, the set of simulated driving scenarios in the map using the initial physical states of the objects in the set of objects; selecting, using the at least one processor, samples of the simulated driving scenarios; training, using the at least one processor, a machine learning model using the selected samples; and operating, using a control circuit, a vehicle in an environment using the trained machine learning model.', 'filing_date': 20200824}, {'publication_number': 'US-12008503-B2', 'title': [{'text': 'Sensor risk assessment database', 'language': 'en', 'truncated': False}], 'abstract': 'An example operation may include one or more of receiving sensor data from one or more sensors associated with a building, storing a block including the sensor data to a shared ledger of a blockchain network, the one or more sensors associated with one or more buildings, requesting a risk assessment for the sensor data, by a blockchain node, calculating the risk assessment with one or more machine learning algorithms based on the sensor data, historical sensor blockchain data, importance of the one or more sensors, and a degree of concern related to the sensor data, providing the risk assessment to the blockchain node, and generating an alert in response to the risk assessment above a threshold. An example operation may include one or more of receiving sensor data from one or more sensors associated with a building, storing a block including the sensor data to a shared ledger of a blockchain network, the one or more sensors associated with one or more buildings, requesting a risk assessment for the sensor data, by a blockchain node, calculating the risk assessment with one or more machine learning algorithms based on the sensor data, historical sensor blockchain data, importance of the one or more sensors, and a degree of concern related to the sensor data, providing the risk assessment to the blockchain node, and generating an alert in response to the risk assessment above a threshold.', 'filing_date': 20181107}, {'publication_number': 'CN-109274314-B', 'title': [{'text': 'Machine learning device, servomotor control system, and machine learning method', 'language': 'en', 'truncated': False}, {'text': '机器学习装置、伺服电动机控制装置、伺服电动机控制系统以及机器学习方法', 'language': 'zh', 'truncated': False}], 'abstract': 'The invention provides a machine learning device, a servomotor control system, and a machine learning method. The machine learning device performs machine learning for a servo motor control device provided with a nonlinear friction compensator, and is provided with: a state information acquisition unit that acquires state information from the servo motor control device, the state information including a combination of a servo state including at least a positional deviation and a correction coefficient of the nonlinear friction compensation unit, by causing the servo motor control device to execute a predetermined program; a behavior information output unit that outputs behavior information including adjustment information of a combination of correction coefficients included in the state information to the servo motor control device; a return output unit that outputs a value of a return in reinforcement learning based on the positional deviation included in the status information; and a cost function updating unit for updating the behavior cost function based on the report value, the state information, and the behavior information output by the report output unit.', 'filing_date': 20180713}, {'publication_number': 'KR-102403174-B1', 'title': [{'text': '중요도에 따른 데이터 정제 방법 및 이를 실행시키기 위하여 기록매체에 기록된 컴퓨터 프로그램', 'language': 'ko', 'truncated': False}, {'text': 'Method for data purification according to importance, and computer program recorded on record-medium for executing method therefor', 'language': 'en', 'truncated': False}], 'abstract': 'The present invention proposes a method for cleaning data according to importance, which can clean 2D images pre-collected for artificial intelligence (AI) machine learning according to importance. The method comprises the steps in which: a learning data generating device evaluates importance by analyzing 2D images pre-collected for AI machine learning; and the learning data generating device cleans at least one of the pre-collected 2D images according to the evaluated importance. Therefore, the method can analyze the pre-collected 2D images to evaluate importance, and can clean at least one of the 2D images pre-collected according to the evaluated importance, thereby increasing the efficiency of machine learning as a 2D image with relatively low importance is cleaned.', 'filing_date': 20211221}, {'publication_number': 'US-11907882-B1', 'title': [{'text': 'Model validation of credit risk', 'language': 'en', 'truncated': False}], 'abstract': 'The innovation disclosed and claimed herein, in one aspect thereof, comprises systems and methods of validating models guided by machine learning algorithms. The innovation can begin by receiving a risk model for validation having multiple sets of data. A first data set is selected from as an input. Outputs are generated for validation. One output can be generating a second set of analysis results using a comparable algorithm to the risk model. Another output can be generating a second set of variables and transformations using a machine learning algorithm and an untransformed set of the selected variables to assess the set of selected transformations. Another output can be generating a third set of variables using one or more machine learning algorithms and an extended feature set of variables to assess the selected variables. The outputs are compared to the analysis results, coefficients, selected variables, and selected transformations. A report of the comparison is generated. The innovation disclosed and claimed herein, in one aspect thereof, comprises systems and methods of validating models guided by machine learning algorithms. The innovation can begin by receiving a risk model for validation having multiple sets of data. A first data set is selected from as an input. Outputs are generated for validation. One output can be generating a second set of analysis results using a comparable algorithm to the risk model. Another output can be generating a second set of variables and transformations using a machine learning algorithm and an untransformed set of the selected variables to assess the set of selected transformations. Another output can be generating a third set of variables using one or more machine learning algorithms and an extended feature set of variables to assess the selected variables. The outputs are compared to the analysis results, coefficients, selected variables, and selected transformations. A report of the comparison is generated.', 'filing_date': 20220825}, {'publication_number': 'CN-115643049-A', 'title': [{'text': 'Method for detecting mining action in real time based on encrypted flow analysis', 'language': 'en', 'truncated': False}, {'text': '一种基于加密流量分析的挖矿行为实时检测方法', 'language': 'zh', 'truncated': False}], 'abstract': 'The invention relates to a real-time detection method for mining excavation behaviors based on encrypted flow analysis, and belongs to the technical field of block chain encrypted network flow processing. The method extracts the length of the data packets, the number of the data packets, the arrival time interval of the data packets and the flow duration time of the data packets from the encrypted flow as characteristics, and further combines the characteristics with a machine learning model to realize accurate identification of the mining action. Under the scene that the network flow is encrypted, the method can detect the mining behavior of the encrypted currency, realize the timely blocking of the malicious mining flow and ensure that the equipment resources are prevented from being invaded. The invention only needs to be deployed at the gateway, reduces the implementation cost of a network administrator, and simultaneously only needs to passively monitor the flow without interfering the normal work of the network.', 'filing_date': 20220920}, {'publication_number': 'CN-117875443-A', 'title': [{'text': 'Implementing machine learning in a local APL edge device with power limitation', 'language': 'en', 'truncated': False}, {'text': '在具有功率限制的本地apl边缘设备中实现机器学习', 'language': 'zh', 'truncated': False}], 'abstract': 'A method performed by an Advanced Physical Layer (APL) -based edge device having power limitations is provided. The method includes applying an event driven framework that meets power constraints of an APL-based edge device to receive input data; the method includes applying an event driven framework to input data to invoke a Machine Learning (ML) model trained to analyze the input data and make inferences about one or more aspects of the industrial system based on the input data, and applying the invoked machine learning model to analyze the input data and make inferences about one or more aspects of the industrial system based on the input data. The APL based edge devices receive input data from one or more source field devices of the industrial system and/or use inferences to make decisions and apply actions to the industrial system.', 'filing_date': 20231010}, {'publication_number': 'CN-111444629-A', 'title': [{'text': 'Reinforcing steel bar corrosion parameter prediction method based on support vector machine', 'language': 'en', 'truncated': False}, {'text': '一种基于支持向量机的钢筋锈蚀参数预测方法', 'language': 'zh', 'truncated': False}], 'abstract': 'The invention relates to the technical fields of reinforced concrete, machine learning technology and big data, in particular to a method for predicting steel bar corrosion parameters based on a support vector machine. The steel bar corrosion parameter prediction method based on the support vector machine comprises the following steps: the method comprises the following steps: manufacturing a test piece of the corrosion reinforcing steel bar; step two: 3D scanning is carried out on the test piece, and a 3D image model is generated; step three: according to the 3D image model, calculating the specific characteristic parameters of the section of the test piece and the steel bar corrosion rate corresponding to the specific characteristic parameters of the section; step four: setting specific characteristic parameters of the cross section and the corrosion rate of the steel bar corresponding to the specific characteristic parameters as a group of basic data, inputting the multiple groups of basic data into a support vector machine for learning and training to obtain a prediction model for predicting the corrosion rate of the steel bar to be measured. The method can realize the prediction of the corrosion rate of the steel bar only by knowing the specific characteristic parameters of the corroded steel bar, and has high accuracy and convenient use.', 'filing_date': 20200415}]\n",
            "Patent titles: [[{'text': 'Prudent ensemble models in machine learning with high precision for use in network security', 'language': 'en', 'truncated': False}], [{'text': 'Information processing device, information processing system, information processing method, and storage medium', 'language': 'en', 'truncated': False}, {'text': 'Dispositif de traitement d&#39;informations, système de traitement d&#39;informations, procédé de traitement d&#39;informations et support de stockage', 'language': 'fr', 'truncated': False}, {'text': '情報処理装置、情報処理システム、情報処理方法、及び、記憶媒体', 'language': 'ja', 'truncated': False}], [{'text': 'Driving scenario sampling for training/tuning machine learning models for vehicles', 'language': 'en', 'truncated': False}], [{'text': 'Sensor risk assessment database', 'language': 'en', 'truncated': False}], [{'text': 'Machine learning device, servomotor control system, and machine learning method', 'language': 'en', 'truncated': False}, {'text': '机器学习装置、伺服电动机控制装置、伺服电动机控制系统以及机器学习方法', 'language': 'zh', 'truncated': False}], [{'text': '중요도에 따른 데이터 정제 방법 및 이를 실행시키기 위하여 기록매체에 기록된 컴퓨터 프로그램', 'language': 'ko', 'truncated': False}, {'text': 'Method for data purification according to importance, and computer program recorded on record-medium for executing method therefor', 'language': 'en', 'truncated': False}], [{'text': 'Model validation of credit risk', 'language': 'en', 'truncated': False}], [{'text': 'Method for detecting mining action in real time based on encrypted flow analysis', 'language': 'en', 'truncated': False}, {'text': '一种基于加密流量分析的挖矿行为实时检测方法', 'language': 'zh', 'truncated': False}], [{'text': 'Implementing machine learning in a local APL edge device with power limitation', 'language': 'en', 'truncated': False}, {'text': '在具有功率限制的本地apl边缘设备中实现机器学习', 'language': 'zh', 'truncated': False}], [{'text': 'Reinforcing steel bar corrosion parameter prediction method based on support vector machine', 'language': 'en', 'truncated': False}, {'text': '一种基于支持向量机的钢筋锈蚀参数预测方法', 'language': 'zh', 'truncated': False}]]\n",
            "Refining search with GPT-4. Query: machine learning, Titles: [[{'text': 'Prudent ensemble models in machine learning with high precision for use in network security', 'language': 'en', 'truncated': False}], [{'text': 'Information processing device, information processing system, information processing method, and storage medium', 'language': 'en', 'truncated': False}, {'text': 'Dispositif de traitement d&#39;informations, système de traitement d&#39;informations, procédé de traitement d&#39;informations et support de stockage', 'language': 'fr', 'truncated': False}, {'text': '情報処理装置、情報処理システム、情報処理方法、及び、記憶媒体', 'language': 'ja', 'truncated': False}], [{'text': 'Driving scenario sampling for training/tuning machine learning models for vehicles', 'language': 'en', 'truncated': False}], [{'text': 'Sensor risk assessment database', 'language': 'en', 'truncated': False}], [{'text': 'Machine learning device, servomotor control system, and machine learning method', 'language': 'en', 'truncated': False}, {'text': '机器学习装置、伺服电动机控制装置、伺服电动机控制系统以及机器学习方法', 'language': 'zh', 'truncated': False}], [{'text': '중요도에 따른 데이터 정제 방법 및 이를 실행시키기 위하여 기록매체에 기록된 컴퓨터 프로그램', 'language': 'ko', 'truncated': False}, {'text': 'Method for data purification according to importance, and computer program recorded on record-medium for executing method therefor', 'language': 'en', 'truncated': False}], [{'text': 'Model validation of credit risk', 'language': 'en', 'truncated': False}], [{'text': 'Method for detecting mining action in real time based on encrypted flow analysis', 'language': 'en', 'truncated': False}, {'text': '一种基于加密流量分析的挖矿行为实时检测方法', 'language': 'zh', 'truncated': False}], [{'text': 'Implementing machine learning in a local APL edge device with power limitation', 'language': 'en', 'truncated': False}, {'text': '在具有功率限制的本地apl边缘设备中实现机器学习', 'language': 'zh', 'truncated': False}], [{'text': 'Reinforcing steel bar corrosion parameter prediction method based on support vector machine', 'language': 'en', 'truncated': False}, {'text': '一种基于支持向量机的钢筋锈蚀参数预测方法', 'language': 'zh', 'truncated': False}]]\n",
            "GPT-4 response: 1. \"Prudent ensemble models in machine learning with high precision for use in network security\": This patent is directly related to machine learning models, which is the crux of your patent claim.\n",
            "\n",
            "2. \"Driving scenario sampling for training/tuning machine learning models for vehicles\": This prior art is relevant because it mentions the use of machine learning models which can potentially conflict with your patent claim, depending upon the specifics of your claim.\n",
            "\n",
            "3. \"Machine learning device, servomotor control system, and machine learning method\": As the title suggests, this prior art talks about a machine learning device and a method related to machine learning. Considering your patent claim is about machine learning, this is a significant piece of prior art.\n",
            "\n",
            "4. \"Implementing machine learning in a local APL edge device with power limitation\": This patent talks about the implementation of machine learning on a specific device. It is necessary to determine if this overlaps with your patent claim based on the specifics of the machine learning your claim is based on.\n",
            "\n",
            "5. \"Reinforcing steel bar corrosion parameter prediction method based on support vector machine\": This is a specific application of machine learning in the field of construction, indicating that machine learning techniques were used earlier in this particular field.\n",
            "\n",
            "In conclusion, these prior art references show that machine learning has been applied in a multitude of applications and methods before, and patent examiners might use these references to question the novelty, non-obviousness, or industrial applicability of your patent claim. Therefore, it is necessary to clearly define the distinctiveness of your invention in your patent application.\n",
            "Function 'refine_search_with_gpt' took 16.8518 seconds\n",
            "Function 'search_patents_gpt' took 18.2294 seconds\n",
            "Prior art list from GPT-4: ['1. \"Prudent ensemble models in machine learning with high precision for use in network security\": This patent is directly related to machine learning models, which is the crux of your patent claim.', '', '2. \"Driving scenario sampling for training/tuning machine learning models for vehicles\": This prior art is relevant because it mentions the use of machine learning models which can potentially conflict with your patent claim, depending upon the specifics of your claim.', '', '3. \"Machine learning device, servomotor control system, and machine learning method\": As the title suggests, this prior art talks about a machine learning device and a method related to machine learning. Considering your patent claim is about machine learning, this is a significant piece of prior art.', '', '4. \"Implementing machine learning in a local APL edge device with power limitation\": This patent talks about the implementation of machine learning on a specific device. It is necessary to determine if this overlaps with your patent claim based on the specifics of the machine learning your claim is based on.', '', '5. \"Reinforcing steel bar corrosion parameter prediction method based on support vector machine\": This is a specific application of machine learning in the field of construction, indicating that machine learning techniques were used earlier in this particular field.', '', 'In conclusion, these prior art references show that machine learning has been applied in a multitude of applications and methods before, and patent examiners might use these references to question the novelty, non-obviousness, or industrial applicability of your patent claim. Therefore, it is necessary to clearly define the distinctiveness of your invention in your patent application.']\n",
            "Checking novelty for patent claim: machine learning\n",
            "Comparing with prior art patents: ['1. \"Prudent ensemble models in machine learning with high precision for use in network security\": This patent is directly related to machine learning models, which is the crux of your patent claim.', '', '2. \"Driving scenario sampling for training/tuning machine learning models for vehicles\": This prior art is relevant because it mentions the use of machine learning models which can potentially conflict with your patent claim, depending upon the specifics of your claim.', '', '3. \"Machine learning device, servomotor control system, and machine learning method\": As the title suggests, this prior art talks about a machine learning device and a method related to machine learning. Considering your patent claim is about machine learning, this is a significant piece of prior art.', '', '4. \"Implementing machine learning in a local APL edge device with power limitation\": This patent talks about the implementation of machine learning on a specific device. It is necessary to determine if this overlaps with your patent claim based on the specifics of the machine learning your claim is based on.', '', '5. \"Reinforcing steel bar corrosion parameter prediction method based on support vector machine\": This is a specific application of machine learning in the field of construction, indicating that machine learning techniques were used earlier in this particular field.', '', 'In conclusion, these prior art references show that machine learning has been applied in a multitude of applications and methods before, and patent examiners might use these references to question the novelty, non-obviousness, or industrial applicability of your patent claim. Therefore, it is necessary to clearly define the distinctiveness of your invention in your patent application.']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cosine Similarities: [[0.30278526 0.         0.25443072 0.         0.54345625 0.\n",
            "  0.32235069 0.         0.32233819 0.         0.09713204]]\n",
            "Function 'check_novelty' took 0.5890 seconds\n",
            "Cosine Similarities to Prior Art:  [[0.30278526 0.         0.25443072 0.         0.54345625 0.\n",
            "  0.32235069 0.         0.32233819 0.         0.09713204]]\n",
            "Function 'patent_novelty_check' took 18.8203 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "similarities_flat = similarities.flatten()\n",
        "for i,j in zip(similarities_flat[:5], prior_art_list[:5]):\n",
        "  print(\"similarity is: \", round(i,2))\n",
        "  print(j)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ewjrhC-dt2Kl",
        "outputId": "4336cd6a-413b-4b47-9961-067174a3be35"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "similarity is:  0.3\n",
            "1. \"Prudent ensemble models in machine learning with high precision for use in network security\": This patent is directly related to machine learning models, which is the crux of your patent claim.\n",
            "similarity is:  0.0\n",
            "\n",
            "similarity is:  0.25\n",
            "2. \"Driving scenario sampling for training/tuning machine learning models for vehicles\": This prior art is relevant because it mentions the use of machine learning models which can potentially conflict with your patent claim, depending upon the specifics of your claim.\n",
            "similarity is:  0.0\n",
            "\n",
            "similarity is:  0.54\n",
            "3. \"Machine learning device, servomotor control system, and machine learning method\": As the title suggests, this prior art talks about a machine learning device and a method related to machine learning. Considering your patent claim is about machine learning, this is a significant piece of prior art.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import openai\n",
        "from transformers import pipeline\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from google.colab import userdata\n",
        "import time\n",
        "import os\n",
        "\n",
        "# Set OpenAI API key for GPT-4\n",
        "openai.api_key = userdata.get('OPENAI_API_KEY')\n",
        "os.environ['OPENAI_API_KEY'] = userdata.get('OPENAI_API_KEY')\n",
        "\n",
        "# Timer decorator to track execution time of functions\n",
        "def timer_decorator(func):\n",
        "    def wrapper(*args, **kwargs):\n",
        "        start_time = time.time()\n",
        "        result = func(*args, **kwargs)\n",
        "        end_time = time.time()\n",
        "        print(f\"Function '{func.__name__}' took {end_time - start_time:.4f} seconds\")\n",
        "        return result\n",
        "    return wrapper\n",
        "from openai import OpenAI\n",
        "client = OpenAI()\n",
        "# 1. Patent Claim Analysis for Suggestions using GPT-4\n",
        "@timer_decorator\n",
        "def analyze_patent_claims_for_suggestions(claims):\n",
        "    \"\"\"\n",
        "    Uses GPT-4 to analyze patent claims and suggest ways to broaden or narrow them.\n",
        "    \"\"\"\n",
        "    print(f\"Analyzing patent claims with GPT-4. Claims: {claims}\")\n",
        "\n",
        "    try:\n",
        "        response = client.chat.completions.create(\n",
        "            model=\"gpt-4\",\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": \"You are a patent expert tasked with analyzing and improving patent claims.\"},\n",
        "                {\"role\": \"user\", \"content\": f\"Here are the patent claims:\\n{claims}\\nSuggest ways to broaden or narrow these claims to improve the chances of approval or maximize protection.\"}\n",
        "            ]\n",
        "        )\n",
        "        gpt_result =  response.choices[0].message.content #response.choices[0].message['content']\n",
        "        print(f\"GPT-4 suggestions: {gpt_result}\")\n",
        "        return gpt_result\n",
        "    except Exception as e:\n",
        "        print(f\"Error during GPT-4 API call: {e}\")\n",
        "        return \"Error in GPT-4 processing\"\n",
        "\n",
        "# 2. Example: Upload and Analyze Patent Claims\n",
        "@timer_decorator\n",
        "def upload_and_analyze_patent_claims(file_path):\n",
        "    \"\"\"\n",
        "    Uploads a patent document, extracts claims, and analyzes them for suggestions.\n",
        "    \"\"\"\n",
        "    print(f\"Uploading and analyzing patent claims from file: {file_path}\")\n",
        "\n",
        "    # Mock function to simulate reading and extracting claims from a patent file\n",
        "    with open(file_path, 'r') as file:\n",
        "        claims = file.read()  # Assume the file contains patent claims in plain text\n",
        "\n",
        "    # Analyze the claims using GPT-4\n",
        "    suggestions = analyze_patent_claims_for_suggestions(claims)\n",
        "\n",
        "    return suggestions\n",
        "\n",
        "# 3. Full Pipeline for Patent Claim Suggestions\n",
        "@timer_decorator\n",
        "def patent_claim_suggestion_pipeline(file_path):\n",
        "    \"\"\"\n",
        "    Pipeline to analyze patent claims and provide suggestions using GPT-4.\n",
        "    \"\"\"\n",
        "    print(f\"Starting patent claim suggestion pipeline for file: {file_path}\")\n",
        "\n",
        "    # Step 1: Upload and analyze patent claims\n",
        "    suggestions = upload_and_analyze_patent_claims(file_path)\n",
        "\n",
        "    print(\"Patent Claim Suggestions:\")\n",
        "    print(suggestions)\n",
        "    return suggestions\n",
        "\n",
        "# Example usage:\n",
        "\n",
        "file_path = \"/content/drive/MyDrive/Colab Notebooks/sample_patent.txt\"  # Provide the path to a file containing patent claims\n",
        "patent_claim_suggestion_pipeline(file_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ogO_fu9Yzwkf",
        "outputId": "9a08aa46-5668-4a8a-af1b-9a19388b995b"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting patent claim suggestion pipeline for file: /content/drive/MyDrive/Colab Notebooks/sample_patent.txt\n",
            "Uploading and analyzing patent claims from file: /content/drive/MyDrive/Colab Notebooks/sample_patent.txt\n",
            "Analyzing patent claims with GPT-4. Claims: Description\n",
            "CROSS-REFERENCE TO RELATED APPLICATION\n",
            "This application claims priority under 35 U.S.C. 119 to U.S. Provisional Application No. 61/056,706, entitled: “TECHNIQUES FOR POWER CONVERSION,” filed on May 28, 2008, the contents of which are incorporated herein as if set forth in full.\n",
            "BACKGROUND\n",
            "Generally, magnetic components use magnetic materials for shaping and directing magnetic fields in a manner designed to achieve a desired electrical performance. Magnetic components are readily used in a wide variety of electronic equipment such as computers, televisions, telephones, etc. In operation, magnetic fields may act as the medium for storing, transferring, and releasing electromagnetic energy. Transformers are one specific example of a magnetic component, and typically comprise two or more windings of conductors (e.g., copper wire) wound around a bobbin with a magnetic core inserted through the bobbin. The bobbin may generally be made of a molded plastic or any other suitable dielectric material. The conductors may be wound around the bobbin a predetermined number of times and in a predetermined configuration to achieve specific electrical characteristics. For example, the number of windings (e.g., a primary winding and a secondary winding) and the number of turns for the conductors in each winding may be a function of the intended application for the transformer.\n",
            "To form the magnetic field in the transformer, a core assembly having high magnetic permeability may be inserted into the bobbin. Often the core assembly is made in two pieces, each having an “E” shaped cross-section that may be inserted into opposite ends of the bobbin. The transformer assembly may then be held together by various physical means such as a spring clip, tape, or an adhesive. Of course, different configurations may also be used for various applications.\n",
            "Transformers generally operate on the principle that a change in current flowing through a first winding conductor, which is isolated from a second winding conductor, creates a magnetic flux in a core that causes a change in the current flow in the second winding conductor. The ratio of current in the two winding conductors may generally be related to the relative number of windings of each conductor. This may in turn create a voltage that may be the product of the number of turns multiplied by the change in magnetic flux.\n",
            "Transformers are used in several applications, including power converters (or power adapters) used to power electronic devices, such as cell phones, computers, and the like. One type of power converter is a Switched Mode Power Supplies (SMPS). An SMPS may include a power supply unit and a circuit inside the unit to regulate the current. The regulating circuit may control the current so that it can stabilize it to a set voltage that is then sent to the electronic device. Due of weight, economic, and convenience factors, SMPS's are the devices of choice to power most consumer electronics that need stable current and voltage. However, they must be designed carefully to provide power with acceptable efficiency and minimal noise.\n",
            "To meet these requirements, power converters may include one or more stages that include one or more magnetic components including filters, transformers, inductors, or the like. Many power converters are designed to provide multiple output voltages. A typical example is the desktop ATX computer power supply, which produces 12 V, 5 V, and 3.3 V as well as other supplies. The 12 V, 5 V, and 3.3 V supplies all require tight voltage regulation and must produce a large output current. In order to produce all of the desired output voltages from a single transformer, the turns-ratio of the transformer between the primary and secondary windings should match the input voltage relative to the output voltages plus any rectifier voltage drops in the output stages. In order to keep the transformer secondary turns to a minimum, some error is often introduced into the output voltages due to use of integer turns-ratios in low numbers.\n",
            "As can be appreciated, it may be desirable to have relatively few secondary windings for various reasons. For example, since the voltage may be “stepped down” from the primary windings to the secondary windings (e.g., from 120 V down to 3.3 V), the turns-ratio may be very large, which requires a large number of turns for the primary windings relative to the secondary windings. Second, since the secondary windings may generally carry a relatively large amount of current, windings having a relatively large cross-section may be used, which increases the physical space required by the windings. By utilizing relatively few turns, the physical space required by the secondary windings and the primary windings may be reduced.\n",
            "Some types of AC-to-DC power supplies may include isolation transformers that step a high-voltage bus (e.g. 250 V-400 V) down to one or more low-voltage, high-current outputs. The resulting large turns-ratio in the isolation transformer requires a primary winding that utilizes many turns of relatively small wire and larger, high-current secondary windings that typically contain only a few turns (e.g., less than 10-15 turns). Due to larger current requirements, the secondary windings usually have a relatively large cross-sectional area. The large cross-sectional area of the secondary windings often causes difficulty in winding the transformers and can also lead to significant high-frequency loss due to the skin effect and the proximity effect.\n",
            "Due to the high current density required for high power transformers, litz wire may be utilized to wind secondary windings directly onto the bobbin. Litz wire may provide the flexibility required to maneuver the wire, but can be very costly for some applications. Additionally, the use of litz wire for multiple outputs (e.g., multiple secondary windings) requires significant hand labor and leads to poor overall copper utilization of the available space due to a large percentage of insulation in the litz wire. Additionally, the cost of manufacturing a transformer with litz wire may be relatively high and prone to manufacturing mistakes and errors.\n",
            "FIGS. 1A-1C illustrate a prior art EE-type core assembly 20 and bobbin 10 used to wind an EE transformer 35. The outside diameter of a rim 16 of the bobbin 10 may be approximately the same as the outer diameter of the window area of the core assembly 20. In practice, the rim 16 of the bobbin 10 may have a diameter that is slightly smaller than the window area of the core assembly 20 to insure clearance for assembly of the core assembly 20 onto the bobbin 10 to form the transformer 35. The core assembly 20 may include a top half 22 having two outer legs 26, 28, and a center leg 30 sized to be insertable into a hollow portion of the bobbin 10. The core assembly 20 may also include a bottom half 24 having two outer legs 32, 34, and a center leg 36.\n",
            "The standard method of wrapping a winding (e.g., the winding wire 38 shown in FIG. 38) for the transformer 35 is to attach a wire to a bobbin pin (e.g., a pin 14), wrap the wire around a winding surface 12 of the bobbin 10 as many times as necessary to achieve the desired number of turns, and then terminate the wire on another bobbin pin. This method may be repeated for all of the windings on the transformer 35. Additionally, one or more layers of insulation material 36 may be provided around one or more of the windings for electrical isolation.\n",
            "If the transformer design maximizes the use of the available window area, then the copper winding's outermost diameter may be approximately the same as the outside diameter of the rim 16 of the bobbin 10. In the case of large current-carrying secondary windings, the windings may typically be composed of litz wire or copper foil. In either case, additional termination leads may need to be added to the litz wire or to the copper foil to connect the wire to a bobbin pin. Also, in either case, the windings of the transformer may use as much of the available window area as possible.\n",
            "SUMMARY\n",
            "The following embodiments and aspects of thereof are described and illustrated in conjunction with systems, tools, and methods which are meant to be exemplary and illustrative, and not limiting in scope. In various embodiments, one or more of the above-described problems have been reduced or eliminated, while other embodiments are directed to other improvements.\n",
            "According to a first aspect, a transformer is provided that includes a bobbin having a winding surface and hollow portion configured to receive a portion of a core assembly. The transformer also includes a first winding wrapped around the winding surface, and a second winding disposed over the winding surface. The second winding is sufficiently rigid such that it may be pre-formed prior to assembly of the transformer. The transformer also includes a core assembly disposed proximate to the bobbin, wherein a portion of the core assembly is disposed within the hollow portion of the bobbin.\n",
            "According to a second aspect, a method for forming a transformer is provided. The method includes providing a bobbin including a winding surface and a hollow portion, the hollow portion configured for receiving at least a portion of a core assembly. The method also includes winding a first conductor around the winding surface of the bobbin, and pre-forming a second conductor into a coil configuration that includes one or more turns, the second conductor being sufficiently rigid such that it may substantially maintain a pre-formed shaped without external support. The method, further includes positioning the second conductor over the winding surface by passing the second conductor over one end of the bobbin, and inserting at least a portion of a core assembly into the hollow portion of the bobbin.\n",
            "According to a third aspect, a transformer is provided that includes a bobbin having a winding surface, a hollow portion configured to receive a portion of a core assembly, and a plurality of pins The transformer also includes a first winding wrapped, around the winding surface, and a second winding disposed over the winding surface. The second winding is sufficiently rigid such that it may be pre-formed prior to assembly of the transformer. The transformer further includes a third winding disposed over the winding surface, the third winding being sufficiently rigid such that it may be pre-formed prior to assembly of the transformer. In addition, the second winding and the third winding are interwoven together. The transformer also includes a core assembly disposed proximate to the bobbin, wherein a portion of the core assembly is disposed within the hollow portion of the bobbin. In addition, the transformer includes a circuit board, wherein at least one of the plurality pins of the bobbin is coupled to the circuit board, and wherein at least a portion of the second winding and a portion of the third winding are coupled directly to the circuit board.\n",
            "In addition to the exemplary aspects and embodiments described above, further aspects and embodiments will become apparent by reference to the drawings and by study of the following descriptions.\n",
            "BRIEF DESCRIPTION OF THE DRAWINGS\n",
            "FIGS. 1A-1C illustrate a prior art EE-type transformer.\n",
            "FIG. 2 illustrates an exemplary bobbin that may be included as part of a transformer.\n",
            "FIG. 3 illustrates a plurality of secondary windings that may be positioned over a winding surface of the bobbin shown in FIG. 2 to form a transformer.\n",
            "FIG. 4 illustrates the secondary windings shown in FIG. 3 after they have been interwoven or screwed together.\n",
            "FIG. 5 illustrates a plurality of secondary windings made of a flat wire that may be included as part of a transformer.\n",
            "FIG. 6 illustrates the plurality of secondary windings positioned over a bobbin, both of which may be included as parts of a transformer.\n",
            "FIG. 7 illustrates an exemplary transformer.\n",
            "FIG. 8 illustrates a cross-sectional view of exemplary bobbin that includes primary and secondary windings.\n",
            "DETAILED DESCRIPTION\n",
            "While the invention is susceptible to various modifications and alternative forms, specific embodiments thereof have been shown by way of example in the drawings and are herein described in detail. It should be understood, however, that it is not intended to limit the invention to the particular form disclosed, but rather, the invention is to cover all modifications, equivalents, and alternatives falling within the scope and spirit of the invention as defined by the claims.\n",
            "The aforementioned problems and other problems are solved by the features described herein by providing transformers having bobbins that are constructed in such a way that pre-formed secondary windings (e.g., resembling springs) may be slipped onto the bobbins after a primary winding has been wound onto the bobbin. Additionally, the secondary windings may also serve as a self-leading device. That is, they may not require terminations on pins of the bobbin, which further reduces cost and complexity. The transformer winding techniques described herein may reduce the number of components, and may promote smaller input filters and/or less filtering stages at a fraction of the cost of conventional transformers.\n",
            "In an embodiment shown in FIGS. 2-4, a primary winding 58 is wound on a bobbin 50 using insulated wire, and then insulating tape 54 may be added for further insulation. It is noted that the primary winding 58 alone fills the winding surface of the bobbin 50 to a diameter that is substantially equal to the diameter of a rim 56 of the bobbin 50. That is, the diameter of the rim 56 of the bobbin 50 may be relatively small, compared to conventional bobbins.\n",
            "FIG. 3 illustrates three secondary windings 60, 62, and 64 that may be used to form a transformer. The secondary windings 60, 62, 64 may be preformed as a spring (or coil) in a process separate from wrapping the primary winding 58 on the bobbin 50, thereby reducing the manufacturing time and hand labor operations required (e.g., compared to the time and labor required if litz wire was used). In this regard, the windings 60, 62, 64 may be configured to be somewhat rigid, such that they maintain their pre-formed shape without requiring an external support structure (e.g., a bobbin). Further, the secondary windings 60, 62, and 64 may be used as pins themselves to be inserted directly onto a circuit board (e.g., see FIG. 7), thereby eliminating the need to solder the windings to pins on the bobbin 50, and to then solder the pins of the bobbin 50 to a circuit board.\n",
            "FIG. 3 shows the three secondary windings 60, 62, and 64 next to the bobbin 50, which already has the primary winding (e.g., the primary winding 58 shown in FIG. 2) wound onto it. The secondary windings (or springs) 60, 62, 64 may then be put together as shown in FIG. 4 by “screwing” or interweaving them together. After the secondary windings 60, 62, and 64 are coupled together, they may then be slid over the primary winding 58 that is already wound around the bobbin 50. Once the bobbin 50 and the windings 60, 62, and 64 have been assembled, a core assembly may be inserted into the hollow portions of the bobbin 50 to form a transformer.\n",
            "FIG. 5 illustrates a plurality of strands 70 of flat wire that may also be used to form the secondary windings (or springs) for a transformer. Once the strands 70 have been pre-formed as shown in FIG. 5, they may then be slid onto the bobbin 50 on top of the primary winding, as shown in FIG. 6.\n",
            "As shown in FIG. 7, after sliding the strands 70 of secondary windings (or springs) onto the bobbin 50, an EE core 75 may be placed into the bobbin 50 and the entire structure may be soldered to a circuit board 78 to form a transformer 80. It will be appreciated that the pre-formed strands 70 of the secondary windings may be soldered directly onto the circuit board 78 without needing to utilize any pins of the bobbin 50. Of course, other types of core assemblies (e.g., EI-type core assemblies) may be used as well. Further, other types and numbers of winding may be used.\n",
            "The use of the manufacturing-friendly transformers described herein also allow the creation of winding configurations that would be very difficult to achieve if one used standard winding techniques. An example is shows in FIG. 8, which illustrates a cross-section of a window area for a bobbin 90 that may be part of a transformer. The top layer of windings 92 a-c (i.e., the rectangular shaped bars) may be secondary windings, and the rows below the windings 92 a-d include other winding layers 96 (e.g., other primary or secondary windings). The secondary windings 92 a-c may be composed of flat wires (e.g., such as the strands 70 of flat wires shown in FIGS. 5-7) that are oriented vertically. That is, the windings 92 a-c each have a cross-sectional area that has a major-axis (e.g., oriented vertically in FIG. 8) and a minor-axis (oriented horizontally in FIG. 8), and the minor-axis is parallel to the winding surface of the bobbin 90. As can be appreciated, this winding configuration may easily be achieved with the transformer construction described herein, but using conventional winding techniques, it would be very difficult to wrap the windings 92 a-c in this vertical orientation directly onto the bobbin 90. In addition to the various windings, insulation layers 94 may be included to further isolate the windings from each other and/or other components.\n",
            "While the invention has been illustrated and described in detail in the drawings and foregoing description, such illustration and description is to be considered as exemplary and not restrictive in character. For example, certain embodiments described hereinabove may be combinable with other described embodiments and/or arranged in other ways (e.g., process elements may be performed in other sequences) Accordingly, it should be understood that only the preferred embodiment and variants thereof have been shown and described and that all changes and modifications that come within the spirit of the invention are desired to be protected.\n",
            "\n",
            "\n",
            "Claims (11)\n",
            "\n",
            "\n",
            "1. A method for forming a transformer, the method comprising:\n",
            "providing a bobbin comprising a winding surface and a hollow portion, the hollow portion configured for receiving at least a portion of a core assembly;\n",
            "winding a first conductor around the winding surface of the bobbin;\n",
            "pre-forming a second conductor into a first coil configuration that includes one or more turns, the second conductor being sufficiently rigid to substantially maintain the first pre-formed coil configuration without support by an external structure;\n",
            "positioning the second conductor over the winding surface by passing the second conductor over one end of the bobbin; and\n",
            "inserting at least the portion of the core assembly into the hollow portion of the bobbin.\n",
            "2. The method of claim 1, further comprising:\n",
            "pre-forming a third conductor into a second coil configuration that includes one or more turns, the third conductor being sufficiently rigid to substantially maintain the second pre-formed coil configuration without support by an external structure; and\n",
            "positioning the third conductor over the winding surface by passing the third conductor over one end of the bobbin.\n",
            "3. The method of claim 2, further comprising:\n",
            "interweaving the second conductor and the third conductor together prior to positioning the second conductor and the third conductor over the winding surface.\n",
            "4. The method of claim 1, further comprising:\n",
            "positioning an insulating layer over the winding surface between the first conductor and the second conductor.\n",
            "5. The method of claim 1, wherein the core assembly includes two E-shaped sections.\n",
            "6. The method of claim 1, wherein the core assembly include an E-shaped section and an I-shaped section.\n",
            "7. The method of claim 1, further comprising:\n",
            "coupling a first end and a second end of the second conductor to a circuit board.\n",
            "8. The method of claim 1, wherein the coupling step includes soldering the first end and the second end of the second conductor to the circuit board.\n",
            "9. The method of claim 1, wherein the second conductor has a non-circular cross-sectional area.\n",
            "10. The method of claim 9, wherein the cross-section of the second conductor has a major axis and a minor axis, and wherein the pre-forming step comprises:\n",
            "forming the second conductor into a coil such that the minor axis is substantially parallel to the winding surface when the second conductor is positioned over the winding surface.\n",
            "11. The method of claim 9, further comprising:\n",
            "pre-forming a third conductor into a second coil configuration that includes one or more turns, the third conductor being sufficiently rigid to substantially maintain the second pre-formed coil configuration without support by an external structure, the third conductor having a non-circular cross-sectional area; and\n",
            "interweaving the second conductor and the third conductor together prior to positioning the second conductor and the third conductor over the winding surface.\n",
            "\n",
            "\n",
            "\n",
            "GPT-4 suggestions: To broaden these claims:\n",
            "\n",
            "1. The method may not only apply to transformers, but also all types of electrical components where winding is required, including inductors, motor coils, etc.\n",
            "\n",
            "   Claim 1 prototype: A method for forming an electrical component, the method comprising: providing a bobbin comprises a winding surface and a hollow portion, the hollow portion is configured for receiving a core assembly; winding a first conductor around the winding surface of the bobbin; pre-forming a second conductor...\n",
            "\n",
            "2. Instead of specifying that the bobbin should have a hollow portion configured for receiving the core assembly, claim that the bobbin just has a part that can cooperate with a core assembly in some way.\n",
            "\n",
            "   Claim 1 prototype: A method for forming a transformer, the method comprising: providing a bobbin comprising a winding surface and part for cooperating with a core assembly; ...\n",
            "\n",
            "3. Instead of specifying that the winding is pre-formed into 'a coil configuration', assume that the conductor is just pre-formed.\n",
            "\n",
            "   Claim 1 prototype:... pre-forming a second conductor that is sufficiently rigid to substantially maintain the pre-formed configuration without support by an external structure...\n",
            "\n",
            "To narrow these claims:\n",
            "\n",
            "1. Specify the characteristics of the first conductor wound around the winding surface of the bobbin, for instance, its material, size or insulation status.\n",
            "\n",
            "   Claim 1 prototype: A method for forming a transformer, the method comprising: providing a bobbin comprising a winding surface and a hollow portion, the hollow portion configured for receiving at least a portion of a core assembly; winding an insulated copper conductor...\n",
            "\n",
            "2. Define the way the second conductor is passed over the one end of the bobbin or its relative position to the first conductor.\n",
            "\n",
            "   Claim 1 prototype: ...positioning the second conductor over the winding surface by passing the second conductor over the upper end of the bobbin.\n",
            "\n",
            "3. Clearly state the use of the transformer after it is completed, which provides the purpose or application of such manufacturing method.\n",
            "\n",
            "   Claim 1 prototype: ...A method for forming a transformer used in power supply units...\n",
            "Function 'analyze_patent_claims_for_suggestions' took 25.9860 seconds\n",
            "Function 'upload_and_analyze_patent_claims' took 25.9903 seconds\n",
            "Patent Claim Suggestions:\n",
            "To broaden these claims:\n",
            "\n",
            "1. The method may not only apply to transformers, but also all types of electrical components where winding is required, including inductors, motor coils, etc.\n",
            "\n",
            "   Claim 1 prototype: A method for forming an electrical component, the method comprising: providing a bobbin comprises a winding surface and a hollow portion, the hollow portion is configured for receiving a core assembly; winding a first conductor around the winding surface of the bobbin; pre-forming a second conductor...\n",
            "\n",
            "2. Instead of specifying that the bobbin should have a hollow portion configured for receiving the core assembly, claim that the bobbin just has a part that can cooperate with a core assembly in some way.\n",
            "\n",
            "   Claim 1 prototype: A method for forming a transformer, the method comprising: providing a bobbin comprising a winding surface and part for cooperating with a core assembly; ...\n",
            "\n",
            "3. Instead of specifying that the winding is pre-formed into 'a coil configuration', assume that the conductor is just pre-formed.\n",
            "\n",
            "   Claim 1 prototype:... pre-forming a second conductor that is sufficiently rigid to substantially maintain the pre-formed configuration without support by an external structure...\n",
            "\n",
            "To narrow these claims:\n",
            "\n",
            "1. Specify the characteristics of the first conductor wound around the winding surface of the bobbin, for instance, its material, size or insulation status.\n",
            "\n",
            "   Claim 1 prototype: A method for forming a transformer, the method comprising: providing a bobbin comprising a winding surface and a hollow portion, the hollow portion configured for receiving at least a portion of a core assembly; winding an insulated copper conductor...\n",
            "\n",
            "2. Define the way the second conductor is passed over the one end of the bobbin or its relative position to the first conductor.\n",
            "\n",
            "   Claim 1 prototype: ...positioning the second conductor over the winding surface by passing the second conductor over the upper end of the bobbin.\n",
            "\n",
            "3. Clearly state the use of the transformer after it is completed, which provides the purpose or application of such manufacturing method.\n",
            "\n",
            "   Claim 1 prototype: ...A method for forming a transformer used in power supply units...\n",
            "Function 'patent_claim_suggestion_pipeline' took 25.9915 seconds\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"To broaden these claims:\\n\\n1. The method may not only apply to transformers, but also all types of electrical components where winding is required, including inductors, motor coils, etc.\\n\\n   Claim 1 prototype: A method for forming an electrical component, the method comprising: providing a bobbin comprises a winding surface and a hollow portion, the hollow portion is configured for receiving a core assembly; winding a first conductor around the winding surface of the bobbin; pre-forming a second conductor...\\n\\n2. Instead of specifying that the bobbin should have a hollow portion configured for receiving the core assembly, claim that the bobbin just has a part that can cooperate with a core assembly in some way.\\n\\n   Claim 1 prototype: A method for forming a transformer, the method comprising: providing a bobbin comprising a winding surface and part for cooperating with a core assembly; ...\\n\\n3. Instead of specifying that the winding is pre-formed into 'a coil configuration', assume that the conductor is just pre-formed.\\n\\n   Claim 1 prototype:... pre-forming a second conductor that is sufficiently rigid to substantially maintain the pre-formed configuration without support by an external structure...\\n\\nTo narrow these claims:\\n\\n1. Specify the characteristics of the first conductor wound around the winding surface of the bobbin, for instance, its material, size or insulation status.\\n\\n   Claim 1 prototype: A method for forming a transformer, the method comprising: providing a bobbin comprising a winding surface and a hollow portion, the hollow portion configured for receiving at least a portion of a core assembly; winding an insulated copper conductor...\\n\\n2. Define the way the second conductor is passed over the one end of the bobbin or its relative position to the first conductor.\\n\\n   Claim 1 prototype: ...positioning the second conductor over the winding surface by passing the second conductor over the upper end of the bobbin.\\n\\n3. Clearly state the use of the transformer after it is completed, which provides the purpose or application of such manufacturing method.\\n\\n   Claim 1 prototype: ...A method for forming a transformer used in power supply units...\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import openai\n",
        "from transformers import pipeline\n",
        "from google.colab import userdata\n",
        "import time\n",
        "import os\n",
        "\n",
        "# Set OpenAI API key for GPT-4\n",
        "openai.api_key = userdata.get('OPENAI_API_KEY')\n",
        "os.environ['OPENAI_API_KEY'] = userdata.get('OPENAI_API_KEY')\n",
        "\n",
        "# Timer decorator to track execution time of functions\n",
        "def timer_decorator(func):\n",
        "    def wrapper(*args, **kwargs):\n",
        "        start_time = time.time()\n",
        "        result = func(*args, **kwargs)\n",
        "        end_time = time.time()\n",
        "        print(f\"Function '{func.__name__}' took {end_time - start_time:.4f} seconds\")\n",
        "        return result\n",
        "    return wrapper\n",
        "from openai import OpenAI\n",
        "client = OpenAI()\n",
        "# 1. Licensing Opportunities & Patent Value Analysis using GPT-4\n",
        "@timer_decorator\n",
        "def analyze_patent_value_and_opportunities(claims, market_data, patent_applications):\n",
        "    \"\"\"\n",
        "    Uses GPT-4 to analyze patent claims and find licensing opportunities and market trends.\n",
        "    Evaluates the patent value in the context of market trends and potential applications.\n",
        "    \"\"\"\n",
        "    print(f\"Analyzing patent claims with GPT-4. Claims: {claims}\")\n",
        "\n",
        "    try:\n",
        "        response = client.chat.completions.create(\n",
        "            model=\"gpt-4\",\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": \"You are a patent expert tasked with evaluating the value of a patent and identifying licensing opportunities.\"},\n",
        "                {\"role\": \"user\", \"content\": f\"Here are the patent claims:\\n{claims}\\nHere is relevant market data:\\n{market_data}\\nHere are some potential applications:\\n{patent_applications}\\nPlease suggest licensing opportunities and evaluate the value of this patent.\"}\n",
        "            ]\n",
        "        )\n",
        "        gpt_result = response.choices[0].message.content #response.choices[0].message.content\n",
        "        print(f\"GPT-4 Licensing Opportunities & Patent Value Suggestions: {gpt_result}\")\n",
        "        return gpt_result\n",
        "    except Exception as e:\n",
        "        print(f\"Error during GPT-4 API call: {e}\")\n",
        "        return \"Error in GPT-4 processing\"\n",
        "\n",
        "# 2. Example: Upload and Analyze Patent Claims for Licensing and Value\n",
        "@timer_decorator\n",
        "def upload_and_analyze_patent_for_licensing(file_path, market_data, patent_applications):\n",
        "    \"\"\"\n",
        "    Uploads a patent document, extracts claims, and analyzes them for licensing opportunities and market value.\n",
        "    \"\"\"\n",
        "    print(f\"Uploading and analyzing patent claims from file: {file_path}\")\n",
        "\n",
        "    # Mock function to simulate reading and extracting claims from a patent file\n",
        "    with open(file_path, 'r') as file:\n",
        "        claims = file.read()  # Assume the file contains patent claims in plain text\n",
        "\n",
        "    # Analyze the claims using GPT-4\n",
        "    suggestions = analyze_patent_value_and_opportunities(claims, market_data, patent_applications)\n",
        "\n",
        "    return suggestions\n",
        "\n",
        "# 3. Full Pipeline for Licensing and Market Value Analysis\n",
        "@timer_decorator\n",
        "def patent_licensing_opportunities_pipeline(file_path, market_data, patent_applications):\n",
        "    \"\"\"\n",
        "    Pipeline to analyze patent claims for licensing opportunities and evaluate their market value using GPT-4.\n",
        "    \"\"\"\n",
        "    print(f\"Starting patent licensing and value analysis pipeline for file: {file_path}\")\n",
        "\n",
        "    # Step 1: Upload and analyze patent claims\n",
        "    suggestions = upload_and_analyze_patent_for_licensing(file_path, market_data, patent_applications)\n",
        "\n",
        "    print(\"Patent Licensing Opportunities & Market Value Suggestions:\")\n",
        "    print(suggestions)\n",
        "    return suggestions\n",
        "\n",
        "# Example usage:\n",
        "file_path = \"/content/drive/MyDrive/Colab Notebooks/sample_patent.txt\"  # Provide the path to a file containing patent claims\n",
        "market_data = \"\"\"\n",
        "- Increasing demand for renewable energy solutions.\n",
        "- Emerging market for AI-driven technologies in healthcare.\n",
        "- Government incentives for energy-efficient technologies.\n",
        "\"\"\"\n",
        "patent_applications = \"\"\"\n",
        "- Application in AI-powered medical diagnostics.\n",
        "- Potential use in energy-efficient smart grids.\n",
        "- Licensing to renewable energy solution providers.\n",
        "\"\"\"\n",
        "patent_licensing_opportunities_pipeline(file_path, market_data, patent_applications)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ITPFmwoq4dYn",
        "outputId": "d68f677b-f74a-4b66-cd18-8f0130cca134"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting patent licensing and value analysis pipeline for file: /content/drive/MyDrive/Colab Notebooks/sample_patent.txt\n",
            "Uploading and analyzing patent claims from file: /content/drive/MyDrive/Colab Notebooks/sample_patent.txt\n",
            "Analyzing patent claims with GPT-4. Claims: Description\n",
            "CROSS-REFERENCE TO RELATED APPLICATION\n",
            "This application claims priority under 35 U.S.C. 119 to U.S. Provisional Application No. 61/056,706, entitled: “TECHNIQUES FOR POWER CONVERSION,” filed on May 28, 2008, the contents of which are incorporated herein as if set forth in full.\n",
            "BACKGROUND\n",
            "Generally, magnetic components use magnetic materials for shaping and directing magnetic fields in a manner designed to achieve a desired electrical performance. Magnetic components are readily used in a wide variety of electronic equipment such as computers, televisions, telephones, etc. In operation, magnetic fields may act as the medium for storing, transferring, and releasing electromagnetic energy. Transformers are one specific example of a magnetic component, and typically comprise two or more windings of conductors (e.g., copper wire) wound around a bobbin with a magnetic core inserted through the bobbin. The bobbin may generally be made of a molded plastic or any other suitable dielectric material. The conductors may be wound around the bobbin a predetermined number of times and in a predetermined configuration to achieve specific electrical characteristics. For example, the number of windings (e.g., a primary winding and a secondary winding) and the number of turns for the conductors in each winding may be a function of the intended application for the transformer.\n",
            "To form the magnetic field in the transformer, a core assembly having high magnetic permeability may be inserted into the bobbin. Often the core assembly is made in two pieces, each having an “E” shaped cross-section that may be inserted into opposite ends of the bobbin. The transformer assembly may then be held together by various physical means such as a spring clip, tape, or an adhesive. Of course, different configurations may also be used for various applications.\n",
            "Transformers generally operate on the principle that a change in current flowing through a first winding conductor, which is isolated from a second winding conductor, creates a magnetic flux in a core that causes a change in the current flow in the second winding conductor. The ratio of current in the two winding conductors may generally be related to the relative number of windings of each conductor. This may in turn create a voltage that may be the product of the number of turns multiplied by the change in magnetic flux.\n",
            "Transformers are used in several applications, including power converters (or power adapters) used to power electronic devices, such as cell phones, computers, and the like. One type of power converter is a Switched Mode Power Supplies (SMPS). An SMPS may include a power supply unit and a circuit inside the unit to regulate the current. The regulating circuit may control the current so that it can stabilize it to a set voltage that is then sent to the electronic device. Due of weight, economic, and convenience factors, SMPS's are the devices of choice to power most consumer electronics that need stable current and voltage. However, they must be designed carefully to provide power with acceptable efficiency and minimal noise.\n",
            "To meet these requirements, power converters may include one or more stages that include one or more magnetic components including filters, transformers, inductors, or the like. Many power converters are designed to provide multiple output voltages. A typical example is the desktop ATX computer power supply, which produces 12 V, 5 V, and 3.3 V as well as other supplies. The 12 V, 5 V, and 3.3 V supplies all require tight voltage regulation and must produce a large output current. In order to produce all of the desired output voltages from a single transformer, the turns-ratio of the transformer between the primary and secondary windings should match the input voltage relative to the output voltages plus any rectifier voltage drops in the output stages. In order to keep the transformer secondary turns to a minimum, some error is often introduced into the output voltages due to use of integer turns-ratios in low numbers.\n",
            "As can be appreciated, it may be desirable to have relatively few secondary windings for various reasons. For example, since the voltage may be “stepped down” from the primary windings to the secondary windings (e.g., from 120 V down to 3.3 V), the turns-ratio may be very large, which requires a large number of turns for the primary windings relative to the secondary windings. Second, since the secondary windings may generally carry a relatively large amount of current, windings having a relatively large cross-section may be used, which increases the physical space required by the windings. By utilizing relatively few turns, the physical space required by the secondary windings and the primary windings may be reduced.\n",
            "Some types of AC-to-DC power supplies may include isolation transformers that step a high-voltage bus (e.g. 250 V-400 V) down to one or more low-voltage, high-current outputs. The resulting large turns-ratio in the isolation transformer requires a primary winding that utilizes many turns of relatively small wire and larger, high-current secondary windings that typically contain only a few turns (e.g., less than 10-15 turns). Due to larger current requirements, the secondary windings usually have a relatively large cross-sectional area. The large cross-sectional area of the secondary windings often causes difficulty in winding the transformers and can also lead to significant high-frequency loss due to the skin effect and the proximity effect.\n",
            "Due to the high current density required for high power transformers, litz wire may be utilized to wind secondary windings directly onto the bobbin. Litz wire may provide the flexibility required to maneuver the wire, but can be very costly for some applications. Additionally, the use of litz wire for multiple outputs (e.g., multiple secondary windings) requires significant hand labor and leads to poor overall copper utilization of the available space due to a large percentage of insulation in the litz wire. Additionally, the cost of manufacturing a transformer with litz wire may be relatively high and prone to manufacturing mistakes and errors.\n",
            "FIGS. 1A-1C illustrate a prior art EE-type core assembly 20 and bobbin 10 used to wind an EE transformer 35. The outside diameter of a rim 16 of the bobbin 10 may be approximately the same as the outer diameter of the window area of the core assembly 20. In practice, the rim 16 of the bobbin 10 may have a diameter that is slightly smaller than the window area of the core assembly 20 to insure clearance for assembly of the core assembly 20 onto the bobbin 10 to form the transformer 35. The core assembly 20 may include a top half 22 having two outer legs 26, 28, and a center leg 30 sized to be insertable into a hollow portion of the bobbin 10. The core assembly 20 may also include a bottom half 24 having two outer legs 32, 34, and a center leg 36.\n",
            "The standard method of wrapping a winding (e.g., the winding wire 38 shown in FIG. 38) for the transformer 35 is to attach a wire to a bobbin pin (e.g., a pin 14), wrap the wire around a winding surface 12 of the bobbin 10 as many times as necessary to achieve the desired number of turns, and then terminate the wire on another bobbin pin. This method may be repeated for all of the windings on the transformer 35. Additionally, one or more layers of insulation material 36 may be provided around one or more of the windings for electrical isolation.\n",
            "If the transformer design maximizes the use of the available window area, then the copper winding's outermost diameter may be approximately the same as the outside diameter of the rim 16 of the bobbin 10. In the case of large current-carrying secondary windings, the windings may typically be composed of litz wire or copper foil. In either case, additional termination leads may need to be added to the litz wire or to the copper foil to connect the wire to a bobbin pin. Also, in either case, the windings of the transformer may use as much of the available window area as possible.\n",
            "SUMMARY\n",
            "The following embodiments and aspects of thereof are described and illustrated in conjunction with systems, tools, and methods which are meant to be exemplary and illustrative, and not limiting in scope. In various embodiments, one or more of the above-described problems have been reduced or eliminated, while other embodiments are directed to other improvements.\n",
            "According to a first aspect, a transformer is provided that includes a bobbin having a winding surface and hollow portion configured to receive a portion of a core assembly. The transformer also includes a first winding wrapped around the winding surface, and a second winding disposed over the winding surface. The second winding is sufficiently rigid such that it may be pre-formed prior to assembly of the transformer. The transformer also includes a core assembly disposed proximate to the bobbin, wherein a portion of the core assembly is disposed within the hollow portion of the bobbin.\n",
            "According to a second aspect, a method for forming a transformer is provided. The method includes providing a bobbin including a winding surface and a hollow portion, the hollow portion configured for receiving at least a portion of a core assembly. The method also includes winding a first conductor around the winding surface of the bobbin, and pre-forming a second conductor into a coil configuration that includes one or more turns, the second conductor being sufficiently rigid such that it may substantially maintain a pre-formed shaped without external support. The method, further includes positioning the second conductor over the winding surface by passing the second conductor over one end of the bobbin, and inserting at least a portion of a core assembly into the hollow portion of the bobbin.\n",
            "According to a third aspect, a transformer is provided that includes a bobbin having a winding surface, a hollow portion configured to receive a portion of a core assembly, and a plurality of pins The transformer also includes a first winding wrapped, around the winding surface, and a second winding disposed over the winding surface. The second winding is sufficiently rigid such that it may be pre-formed prior to assembly of the transformer. The transformer further includes a third winding disposed over the winding surface, the third winding being sufficiently rigid such that it may be pre-formed prior to assembly of the transformer. In addition, the second winding and the third winding are interwoven together. The transformer also includes a core assembly disposed proximate to the bobbin, wherein a portion of the core assembly is disposed within the hollow portion of the bobbin. In addition, the transformer includes a circuit board, wherein at least one of the plurality pins of the bobbin is coupled to the circuit board, and wherein at least a portion of the second winding and a portion of the third winding are coupled directly to the circuit board.\n",
            "In addition to the exemplary aspects and embodiments described above, further aspects and embodiments will become apparent by reference to the drawings and by study of the following descriptions.\n",
            "BRIEF DESCRIPTION OF THE DRAWINGS\n",
            "FIGS. 1A-1C illustrate a prior art EE-type transformer.\n",
            "FIG. 2 illustrates an exemplary bobbin that may be included as part of a transformer.\n",
            "FIG. 3 illustrates a plurality of secondary windings that may be positioned over a winding surface of the bobbin shown in FIG. 2 to form a transformer.\n",
            "FIG. 4 illustrates the secondary windings shown in FIG. 3 after they have been interwoven or screwed together.\n",
            "FIG. 5 illustrates a plurality of secondary windings made of a flat wire that may be included as part of a transformer.\n",
            "FIG. 6 illustrates the plurality of secondary windings positioned over a bobbin, both of which may be included as parts of a transformer.\n",
            "FIG. 7 illustrates an exemplary transformer.\n",
            "FIG. 8 illustrates a cross-sectional view of exemplary bobbin that includes primary and secondary windings.\n",
            "DETAILED DESCRIPTION\n",
            "While the invention is susceptible to various modifications and alternative forms, specific embodiments thereof have been shown by way of example in the drawings and are herein described in detail. It should be understood, however, that it is not intended to limit the invention to the particular form disclosed, but rather, the invention is to cover all modifications, equivalents, and alternatives falling within the scope and spirit of the invention as defined by the claims.\n",
            "The aforementioned problems and other problems are solved by the features described herein by providing transformers having bobbins that are constructed in such a way that pre-formed secondary windings (e.g., resembling springs) may be slipped onto the bobbins after a primary winding has been wound onto the bobbin. Additionally, the secondary windings may also serve as a self-leading device. That is, they may not require terminations on pins of the bobbin, which further reduces cost and complexity. The transformer winding techniques described herein may reduce the number of components, and may promote smaller input filters and/or less filtering stages at a fraction of the cost of conventional transformers.\n",
            "In an embodiment shown in FIGS. 2-4, a primary winding 58 is wound on a bobbin 50 using insulated wire, and then insulating tape 54 may be added for further insulation. It is noted that the primary winding 58 alone fills the winding surface of the bobbin 50 to a diameter that is substantially equal to the diameter of a rim 56 of the bobbin 50. That is, the diameter of the rim 56 of the bobbin 50 may be relatively small, compared to conventional bobbins.\n",
            "FIG. 3 illustrates three secondary windings 60, 62, and 64 that may be used to form a transformer. The secondary windings 60, 62, 64 may be preformed as a spring (or coil) in a process separate from wrapping the primary winding 58 on the bobbin 50, thereby reducing the manufacturing time and hand labor operations required (e.g., compared to the time and labor required if litz wire was used). In this regard, the windings 60, 62, 64 may be configured to be somewhat rigid, such that they maintain their pre-formed shape without requiring an external support structure (e.g., a bobbin). Further, the secondary windings 60, 62, and 64 may be used as pins themselves to be inserted directly onto a circuit board (e.g., see FIG. 7), thereby eliminating the need to solder the windings to pins on the bobbin 50, and to then solder the pins of the bobbin 50 to a circuit board.\n",
            "FIG. 3 shows the three secondary windings 60, 62, and 64 next to the bobbin 50, which already has the primary winding (e.g., the primary winding 58 shown in FIG. 2) wound onto it. The secondary windings (or springs) 60, 62, 64 may then be put together as shown in FIG. 4 by “screwing” or interweaving them together. After the secondary windings 60, 62, and 64 are coupled together, they may then be slid over the primary winding 58 that is already wound around the bobbin 50. Once the bobbin 50 and the windings 60, 62, and 64 have been assembled, a core assembly may be inserted into the hollow portions of the bobbin 50 to form a transformer.\n",
            "FIG. 5 illustrates a plurality of strands 70 of flat wire that may also be used to form the secondary windings (or springs) for a transformer. Once the strands 70 have been pre-formed as shown in FIG. 5, they may then be slid onto the bobbin 50 on top of the primary winding, as shown in FIG. 6.\n",
            "As shown in FIG. 7, after sliding the strands 70 of secondary windings (or springs) onto the bobbin 50, an EE core 75 may be placed into the bobbin 50 and the entire structure may be soldered to a circuit board 78 to form a transformer 80. It will be appreciated that the pre-formed strands 70 of the secondary windings may be soldered directly onto the circuit board 78 without needing to utilize any pins of the bobbin 50. Of course, other types of core assemblies (e.g., EI-type core assemblies) may be used as well. Further, other types and numbers of winding may be used.\n",
            "The use of the manufacturing-friendly transformers described herein also allow the creation of winding configurations that would be very difficult to achieve if one used standard winding techniques. An example is shows in FIG. 8, which illustrates a cross-section of a window area for a bobbin 90 that may be part of a transformer. The top layer of windings 92 a-c (i.e., the rectangular shaped bars) may be secondary windings, and the rows below the windings 92 a-d include other winding layers 96 (e.g., other primary or secondary windings). The secondary windings 92 a-c may be composed of flat wires (e.g., such as the strands 70 of flat wires shown in FIGS. 5-7) that are oriented vertically. That is, the windings 92 a-c each have a cross-sectional area that has a major-axis (e.g., oriented vertically in FIG. 8) and a minor-axis (oriented horizontally in FIG. 8), and the minor-axis is parallel to the winding surface of the bobbin 90. As can be appreciated, this winding configuration may easily be achieved with the transformer construction described herein, but using conventional winding techniques, it would be very difficult to wrap the windings 92 a-c in this vertical orientation directly onto the bobbin 90. In addition to the various windings, insulation layers 94 may be included to further isolate the windings from each other and/or other components.\n",
            "While the invention has been illustrated and described in detail in the drawings and foregoing description, such illustration and description is to be considered as exemplary and not restrictive in character. For example, certain embodiments described hereinabove may be combinable with other described embodiments and/or arranged in other ways (e.g., process elements may be performed in other sequences) Accordingly, it should be understood that only the preferred embodiment and variants thereof have been shown and described and that all changes and modifications that come within the spirit of the invention are desired to be protected.\n",
            "\n",
            "\n",
            "Claims (11)\n",
            "\n",
            "\n",
            "1. A method for forming a transformer, the method comprising:\n",
            "providing a bobbin comprising a winding surface and a hollow portion, the hollow portion configured for receiving at least a portion of a core assembly;\n",
            "winding a first conductor around the winding surface of the bobbin;\n",
            "pre-forming a second conductor into a first coil configuration that includes one or more turns, the second conductor being sufficiently rigid to substantially maintain the first pre-formed coil configuration without support by an external structure;\n",
            "positioning the second conductor over the winding surface by passing the second conductor over one end of the bobbin; and\n",
            "inserting at least the portion of the core assembly into the hollow portion of the bobbin.\n",
            "2. The method of claim 1, further comprising:\n",
            "pre-forming a third conductor into a second coil configuration that includes one or more turns, the third conductor being sufficiently rigid to substantially maintain the second pre-formed coil configuration without support by an external structure; and\n",
            "positioning the third conductor over the winding surface by passing the third conductor over one end of the bobbin.\n",
            "3. The method of claim 2, further comprising:\n",
            "interweaving the second conductor and the third conductor together prior to positioning the second conductor and the third conductor over the winding surface.\n",
            "4. The method of claim 1, further comprising:\n",
            "positioning an insulating layer over the winding surface between the first conductor and the second conductor.\n",
            "5. The method of claim 1, wherein the core assembly includes two E-shaped sections.\n",
            "6. The method of claim 1, wherein the core assembly include an E-shaped section and an I-shaped section.\n",
            "7. The method of claim 1, further comprising:\n",
            "coupling a first end and a second end of the second conductor to a circuit board.\n",
            "8. The method of claim 1, wherein the coupling step includes soldering the first end and the second end of the second conductor to the circuit board.\n",
            "9. The method of claim 1, wherein the second conductor has a non-circular cross-sectional area.\n",
            "10. The method of claim 9, wherein the cross-section of the second conductor has a major axis and a minor axis, and wherein the pre-forming step comprises:\n",
            "forming the second conductor into a coil such that the minor axis is substantially parallel to the winding surface when the second conductor is positioned over the winding surface.\n",
            "11. The method of claim 9, further comprising:\n",
            "pre-forming a third conductor into a second coil configuration that includes one or more turns, the third conductor being sufficiently rigid to substantially maintain the second pre-formed coil configuration without support by an external structure, the third conductor having a non-circular cross-sectional area; and\n",
            "interweaving the second conductor and the third conductor together prior to positioning the second conductor and the third conductor over the winding surface.\n",
            "\n",
            "\n",
            "\n",
            "GPT-4 Licensing Opportunities & Patent Value Suggestions: This patent describes a method for forming a transformer with preformed secondary windings, reducing the number of components, manufacturing time, and hand labor operations compared to conventional transformers. It also seems to reduce high-frequency loss problems common with use of litz wire and copper foil in winding transformers. The innovation can be used in power converts, transformers, inductors and is particualry beneficial in electronic devices like cell phones, computers, TVs, as well as AI-powered devices and renewable energy solutions.\n",
            "\n",
            "Licensing opportunities:\n",
            "\n",
            "1. Electronics manufacturers: Given the universal use of transformers in various electronic equipment such as computers, televisions, telephones, etc., companies that manufacture these devices could benefit from the efficient manufacturing and optimized design features of the transformer outlined in the patent.\n",
            "   \n",
            "2. Power Supply Companies: Companies that design and manufacture switched mode power supplies (SMPS) for consumer electronics, as well as providers of desktop ATX computer power supplies, could optimize their current solutions with this patent's approach.\n",
            "\n",
            "3. Renewable Energy Technology Providers: This patent can improve energy efficiency, reduce size and weight of transformers, all of which are highly desirable in renewable energy power solutions. Licensing this patent to renewable energy technology providers can allow more effective and efficient transformers in solar panel inverters, wind energy converters etc. \n",
            "\n",
            "4. AI-Powered Medical Diagnostics Manufacturers: As healthcare is moving towards smart and connected technologies, the patented technology may improve the energy efficiency and reliability of AI-powered devices in healthtech. \n",
            "\n",
            "In terms of the patent's value, considering the ubiquity of transformers in electronic devices and the need for improved efficiency and reliability, beyond the cost savings from manufacturing, the technology presents a significant opportunity to enhance product performance and sustainability. Given the growth trajectory within the electronics, energy efficiency, and renewable energy sectors, the potential market size is vast, making the patent highly valuable. The patent value should be further evaluated considering the potential licensing fees, reduced production cost, increased market share, and other economic benefits this patent can offer to the potential licensees. However, a full patent valuation would require a more detailed analysis encompassing factors like the remaining lifespan of the patent, litigation risk, the financial health of potential licensees, as well as economic and technological trends in the industry.\n",
            "Function 'analyze_patent_value_and_opportunities' took 22.1010 seconds\n",
            "Function 'upload_and_analyze_patent_for_licensing' took 22.1080 seconds\n",
            "Patent Licensing Opportunities & Market Value Suggestions:\n",
            "This patent describes a method for forming a transformer with preformed secondary windings, reducing the number of components, manufacturing time, and hand labor operations compared to conventional transformers. It also seems to reduce high-frequency loss problems common with use of litz wire and copper foil in winding transformers. The innovation can be used in power converts, transformers, inductors and is particualry beneficial in electronic devices like cell phones, computers, TVs, as well as AI-powered devices and renewable energy solutions.\n",
            "\n",
            "Licensing opportunities:\n",
            "\n",
            "1. Electronics manufacturers: Given the universal use of transformers in various electronic equipment such as computers, televisions, telephones, etc., companies that manufacture these devices could benefit from the efficient manufacturing and optimized design features of the transformer outlined in the patent.\n",
            "   \n",
            "2. Power Supply Companies: Companies that design and manufacture switched mode power supplies (SMPS) for consumer electronics, as well as providers of desktop ATX computer power supplies, could optimize their current solutions with this patent's approach.\n",
            "\n",
            "3. Renewable Energy Technology Providers: This patent can improve energy efficiency, reduce size and weight of transformers, all of which are highly desirable in renewable energy power solutions. Licensing this patent to renewable energy technology providers can allow more effective and efficient transformers in solar panel inverters, wind energy converters etc. \n",
            "\n",
            "4. AI-Powered Medical Diagnostics Manufacturers: As healthcare is moving towards smart and connected technologies, the patented technology may improve the energy efficiency and reliability of AI-powered devices in healthtech. \n",
            "\n",
            "In terms of the patent's value, considering the ubiquity of transformers in electronic devices and the need for improved efficiency and reliability, beyond the cost savings from manufacturing, the technology presents a significant opportunity to enhance product performance and sustainability. Given the growth trajectory within the electronics, energy efficiency, and renewable energy sectors, the potential market size is vast, making the patent highly valuable. The patent value should be further evaluated considering the potential licensing fees, reduced production cost, increased market share, and other economic benefits this patent can offer to the potential licensees. However, a full patent valuation would require a more detailed analysis encompassing factors like the remaining lifespan of the patent, litigation risk, the financial health of potential licensees, as well as economic and technological trends in the industry.\n",
            "Function 'patent_licensing_opportunities_pipeline' took 22.1112 seconds\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"This patent describes a method for forming a transformer with preformed secondary windings, reducing the number of components, manufacturing time, and hand labor operations compared to conventional transformers. It also seems to reduce high-frequency loss problems common with use of litz wire and copper foil in winding transformers. The innovation can be used in power converts, transformers, inductors and is particualry beneficial in electronic devices like cell phones, computers, TVs, as well as AI-powered devices and renewable energy solutions.\\n\\nLicensing opportunities:\\n\\n1. Electronics manufacturers: Given the universal use of transformers in various electronic equipment such as computers, televisions, telephones, etc., companies that manufacture these devices could benefit from the efficient manufacturing and optimized design features of the transformer outlined in the patent.\\n   \\n2. Power Supply Companies: Companies that design and manufacture switched mode power supplies (SMPS) for consumer electronics, as well as providers of desktop ATX computer power supplies, could optimize their current solutions with this patent's approach.\\n\\n3. Renewable Energy Technology Providers: This patent can improve energy efficiency, reduce size and weight of transformers, all of which are highly desirable in renewable energy power solutions. Licensing this patent to renewable energy technology providers can allow more effective and efficient transformers in solar panel inverters, wind energy converters etc. \\n\\n4. AI-Powered Medical Diagnostics Manufacturers: As healthcare is moving towards smart and connected technologies, the patented technology may improve the energy efficiency and reliability of AI-powered devices in healthtech. \\n\\nIn terms of the patent's value, considering the ubiquity of transformers in electronic devices and the need for improved efficiency and reliability, beyond the cost savings from manufacturing, the technology presents a significant opportunity to enhance product performance and sustainability. Given the growth trajectory within the electronics, energy efficiency, and renewable energy sectors, the potential market size is vast, making the patent highly valuable. The patent value should be further evaluated considering the potential licensing fees, reduced production cost, increased market share, and other economic benefits this patent can offer to the potential licensees. However, a full patent valuation would require a more detailed analysis encompassing factors like the remaining lifespan of the patent, litigation risk, the financial health of potential licensees, as well as economic and technological trends in the industry.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import openai\n",
        "from google.colab import userdata\n",
        "import time\n",
        "import os\n",
        "\n",
        "# Set OpenAI API key for GPT-4\n",
        "openai.api_key = userdata.get('OPENAI_API_KEY')\n",
        "os.environ['OPENAI_API_KEY'] = userdata.get('OPENAI_API_KEY')\n",
        "\n",
        "# Timer decorator to track execution time of functions\n",
        "def timer_decorator(func):\n",
        "    def wrapper(*args, **kwargs):\n",
        "        start_time = time.time()\n",
        "        result = func(*args, **kwargs)\n",
        "        end_time = time.time()\n",
        "        print(f\"Function '{func.__name__}' took {end_time - start_time:.4f} seconds\")\n",
        "        return result\n",
        "    return wrapper\n",
        "\n",
        "from openai import OpenAI\n",
        "client = OpenAI()\n",
        "# 1. Generate Abstract using GPT-4\n",
        "@timer_decorator\n",
        "def generate_patent_abstract(technical_description):\n",
        "    \"\"\"\n",
        "    Uses GPT-4 to generate a high-quality patent abstract from the technical description.\n",
        "    \"\"\"\n",
        "    print(f\"Generating patent abstract for: {technical_description}\")\n",
        "\n",
        "    try:\n",
        "        response = client.chat.completions.create(\n",
        "            model=\"gpt-4\",\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": \"You are a patent expert tasked with generating a patent abstract from a technical description.\"},\n",
        "                {\"role\": \"user\", \"content\": f\"Here is the technical description of the invention:\\n{technical_description}\\nPlease write a high-quality patent abstract based on this description.\"}\n",
        "            ]\n",
        "        )\n",
        "        abstract = response.choices[0].message.content# response.choices[0].message.content\n",
        "        print(f\"Generated Patent Abstract: {abstract}\")\n",
        "        return abstract\n",
        "    except Exception as e:\n",
        "        print(f\"Error during GPT-4 API call for abstract generation: {e}\")\n",
        "        return \"Error in GPT-4 processing\"\n",
        "\n",
        "# 2. Generate Detailed Description using GPT-4\n",
        "@timer_decorator\n",
        "def generate_detailed_description(technical_description):\n",
        "    \"\"\"\n",
        "    Uses GPT-4 to generate the detailed description section of a patent application.\n",
        "    \"\"\"\n",
        "    print(f\"Generating detailed description for: {technical_description}\")\n",
        "\n",
        "    try:\n",
        "        response = client.chat.completions.create(\n",
        "            model=\"gpt-4\",\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": \"You are a patent expert tasked with writing the detailed description for a patent application.\"},\n",
        "                {\"role\": \"user\", \"content\": f\"Here is the technical description of the invention:\\n{technical_description}\\nPlease write a detailed description based on this.\"}\n",
        "            ]\n",
        "        )\n",
        "        detailed_description = response.choices[0].message.content#response.choices[0].message.content\n",
        "        print(f\"Generated Detailed Description: {detailed_description}\")\n",
        "        return detailed_description\n",
        "    except Exception as e:\n",
        "        print(f\"Error during GPT-4 API call for detailed description generation: {e}\")\n",
        "        return \"Error in GPT-4 processing\"\n",
        "\n",
        "# 3. Generate Patent Claims using GPT-4\n",
        "@timer_decorator\n",
        "def generate_patent_claims(technical_description):\n",
        "    \"\"\"\n",
        "    Uses GPT-4 to generate patent claims based on the technical description.\n",
        "    \"\"\"\n",
        "    print(f\"Generating patent claims for: {technical_description}\")\n",
        "\n",
        "    try:\n",
        "        response = client.chat.completions.create(\n",
        "            model=\"gpt-4\",\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": \"You are a patent expert tasked with writing claims for a patent application.\"},\n",
        "                {\"role\": \"user\", \"content\": f\"Here is the technical description of the invention:\\n{technical_description}\\nPlease write a set of patent claims based on this description.\"}\n",
        "            ]\n",
        "        )\n",
        "        claims = response.choices[0].message.content # response.choices[0].message.content\n",
        "        print(f\"Generated Patent Claims: {claims}\")\n",
        "        return claims\n",
        "    except Exception as e:\n",
        "        print(f\"Error during GPT-4 API call for claims generation: {e}\")\n",
        "        return \"Error in GPT-4 processing\"\n",
        "\n",
        "# 4. Full Pipeline for Patent Draft Generation\n",
        "@timer_decorator\n",
        "def patent_draft_pipeline(technical_description):\n",
        "    \"\"\"\n",
        "    Pipeline to generate a full patent draft, including the abstract, detailed description, and claims.\n",
        "    Uses GPT-4 for generating high-quality patent drafts from the technical description.\n",
        "    \"\"\"\n",
        "    print(f\"Starting patent draft generation pipeline for technical description: {technical_description}\")\n",
        "\n",
        "    # Step 1: Generate the abstract\n",
        "    abstract = generate_patent_abstract(technical_description)\n",
        "\n",
        "    # Step 2: Generate the detailed description\n",
        "    detailed_description = generate_detailed_description(technical_description)\n",
        "\n",
        "    # Step 3: Generate the patent claims\n",
        "    claims = generate_patent_claims(technical_description)\n",
        "\n",
        "    # Combine all sections into a full patent draft\n",
        "    patent_draft = f\"\"\"\n",
        "    Patent Application Draft:\n",
        "\n",
        "    1. Abstract:\n",
        "    {abstract}\n",
        "\n",
        "    2. Detailed Description:\n",
        "    {detailed_description}\n",
        "\n",
        "    3. Claims:\n",
        "    {claims}\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"Full Patent Draft:\")\n",
        "    print(patent_draft)\n",
        "    return patent_draft\n",
        "\n",
        "# Example usage:\n",
        "technical_description = \"\"\"\n",
        "A system for improving energy efficiency in smart buildings using a combination of AI-driven sensors and machine learning algorithms. The system analyzes real-time data from temperature, lighting, and occupancy sensors to adjust energy usage dynamically, reducing waste while maintaining comfort. The machine learning model continuously learns from the data to improve its predictions and optimize energy savings.\n",
        "\"\"\"\n",
        "patent_draft_pipeline(technical_description)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "wIAclhhQAO6s",
        "outputId": "f0e964bd-32f4-485f-f211-d8879c6f17d8"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting patent draft generation pipeline for technical description: \n",
            "A system for improving energy efficiency in smart buildings using a combination of AI-driven sensors and machine learning algorithms. The system analyzes real-time data from temperature, lighting, and occupancy sensors to adjust energy usage dynamically, reducing waste while maintaining comfort. The machine learning model continuously learns from the data to improve its predictions and optimize energy savings.\n",
            "\n",
            "Generating patent abstract for: \n",
            "A system for improving energy efficiency in smart buildings using a combination of AI-driven sensors and machine learning algorithms. The system analyzes real-time data from temperature, lighting, and occupancy sensors to adjust energy usage dynamically, reducing waste while maintaining comfort. The machine learning model continuously learns from the data to improve its predictions and optimize energy savings.\n",
            "\n",
            "Generated Patent Abstract: The invention pertains to a system for enhancing energy efficiency in smart buildings, employing a blend of AI-driven sensors and machine learning algorithms. The system capitalizes on real-time data gathered from temperature, lighting and occupancy sensors to dynamically adjust energy consumption, mitigating waste while sustaining comfort levels. The system's machine learning model perpetually learns from the accumulated data, enhancing its predictive abilities, and optimizing energy savings, thereby providing a technologically advanced solution for energy conservation and waste reduction in smart buildings.\n",
            "\n",
            "Function 'generate_patent_abstract' took 5.7401 seconds\n",
            "Generating detailed description for: \n",
            "A system for improving energy efficiency in smart buildings using a combination of AI-driven sensors and machine learning algorithms. The system analyzes real-time data from temperature, lighting, and occupancy sensors to adjust energy usage dynamically, reducing waste while maintaining comfort. The machine learning model continuously learns from the data to improve its predictions and optimize energy savings.\n",
            "\n",
            "Generated Detailed Description: Title: Energy Efficiency Improvement System using AI-Driven Sensors and Machine Learning Algorithms in Smart Buildings\n",
            "\n",
            "Detailed Description:\n",
            "\n",
            "The present invention pertains to an energy management system that leverages Artificial Intelligence (AI) and Machine Learning (ML) capabilities to maximize the energy efficiency of smart buildings. \n",
            "\n",
            "The system primarily comprises a network of AI-controlled sensors that constantly monitor varying aspects of the building's environmental conditions. These sensors are specifically designed to track temperature fluctuations, lighting conditions, and occupancy levels within the building. The gathered data is real-time in nature, enabling the system to adjust the building's energy consumption instantaneously and dynamically according to changing conditions.\n",
            "\n",
            "A central processing unit, which could be in the form of a dedicated server or cloud-based platform, receives the real-time data from multiple sensors. The data received are processed, and its patterns are analyzed to extrapolate context-specific adjustments in the energy consumption of the building. Potential energy changes may apply but are not limited to, the building’s heating, ventilation & air conditioning (HVAC) systems, lighting systems, and other appliances or systems that significantly contribute to energy use.\n",
            "\n",
            "At the core of this invention is a sophisticated Machine Learning (ML) algorithm, which uses the continuous flow of real-time sensor data to predict and optimize energy usage. This ML model is designed to learn from the data it processes over time, improving its predictive accuracy consistently. The learning process allows the system to understand energy usage patterns better and provide increasingly efficient energy management solutions.\n",
            "\n",
            "In addition to energy efficiency, the system aims to maintain optimal comfort, offering a balance between reducing energy waste and ensuring a comfortable environment for the building’s occupants. Even while reducing energy consumption, it ensures that the standards of comfort, such as optimal temperature and illumination, are uncompromised.\n",
            "\n",
            "Overall, the system significantly contributes to environmental sustainability by optimizing the use of energy resources in smart buildings while enhancing occupants' comfort. It bridges the gap between technology, comfort, and sustainability, making it a cutting-edge solution in improving energy efficiency in smart buildings.\n",
            "Function 'generate_detailed_description' took 23.5010 seconds\n",
            "Generating patent claims for: \n",
            "A system for improving energy efficiency in smart buildings using a combination of AI-driven sensors and machine learning algorithms. The system analyzes real-time data from temperature, lighting, and occupancy sensors to adjust energy usage dynamically, reducing waste while maintaining comfort. The machine learning model continuously learns from the data to improve its predictions and optimize energy savings.\n",
            "\n",
            "Generated Patent Claims: 1. A system for enhancing energy efficiency in smart buildings, characterized by the integration of AI-driven sensors and machine learning algorithms.\n",
            "\n",
            "2. The system of claim 1, wherein said system utilizes real-time data generated from a plurality of sensors to dynamically adjust energy consumption.\n",
            "\n",
            "3. The system of claim 2, wherein said sensors include temperature sensors, lighting sensors, and occupancy sensors.\n",
            "\n",
            "4. The system of claim 2, wherein the dynamic adjustment of energy usage results in decreasing waste while maintaining optimal comfort levels within the smart building.\n",
            "\n",
            "5. The system of claim 1, wherein the machine learning algorithm continuously learns from the data gathered from said sensors.\n",
            "\n",
            "6. The system of claim 5, wherein said learning leads to the improvement of the system's predictive capability and optimization of energy savings.\n",
            "\n",
            "7. The system of claim 1, wherein the AI-driven sensors and machine learning algorithms work in synchronization to respond to changes in temperature, lighting, and occupancy in real-time.\n",
            "\n",
            "8. The system of claim 1, where the AI-driven sensors allow for efficient and accurate data collection on various environmental conditions within the smart building.\n",
            "\n",
            "9. The system of claim 5, wherein the continuous learning by the machine learning algorithm allows for a self-improving system.\n",
            "\n",
            "10. A method for enhancing energy efficiency in smart buildings, characterized by a continuous analysis performed by a system which deploys a combination of AI-driven sensors and machine learning algorithms. \n",
            "\n",
            "11. The method of claim 10, wherein said analysis entails real-time data from temperature, lighting, and occupancy sensors to dynamically adjust energy usage.\n",
            "\n",
            "12. The method of claim 10, where through constant learning from the data, the machine learning algorithm improves its predictions, thereby optimizing energy savings.\n",
            "\n",
            "13. The method of claim 10, which decreases waste while maintaining comfort, by dynamically adjusting energy usage based on the real-time data analysis. \n",
            "\n",
            "14. The method of claim 10, which involves providing ongoing, real-time response and adjustment to changes in temperature, lighting, and occupancy, thus maximizing the efficiency and savings in the energy usage.\n",
            "\n",
            "Function 'generate_patent_claims' took 24.7934 seconds\n",
            "Full Patent Draft:\n",
            "\n",
            "    Patent Application Draft:\n",
            "\n",
            "    1. Abstract:\n",
            "    The invention pertains to a system for enhancing energy efficiency in smart buildings, employing a blend of AI-driven sensors and machine learning algorithms. The system capitalizes on real-time data gathered from temperature, lighting and occupancy sensors to dynamically adjust energy consumption, mitigating waste while sustaining comfort levels. The system's machine learning model perpetually learns from the accumulated data, enhancing its predictive abilities, and optimizing energy savings, thereby providing a technologically advanced solution for energy conservation and waste reduction in smart buildings.\n",
            "\n",
            "\n",
            "    2. Detailed Description:\n",
            "    Title: Energy Efficiency Improvement System using AI-Driven Sensors and Machine Learning Algorithms in Smart Buildings\n",
            "\n",
            "Detailed Description:\n",
            "\n",
            "The present invention pertains to an energy management system that leverages Artificial Intelligence (AI) and Machine Learning (ML) capabilities to maximize the energy efficiency of smart buildings. \n",
            "\n",
            "The system primarily comprises a network of AI-controlled sensors that constantly monitor varying aspects of the building's environmental conditions. These sensors are specifically designed to track temperature fluctuations, lighting conditions, and occupancy levels within the building. The gathered data is real-time in nature, enabling the system to adjust the building's energy consumption instantaneously and dynamically according to changing conditions.\n",
            "\n",
            "A central processing unit, which could be in the form of a dedicated server or cloud-based platform, receives the real-time data from multiple sensors. The data received are processed, and its patterns are analyzed to extrapolate context-specific adjustments in the energy consumption of the building. Potential energy changes may apply but are not limited to, the building’s heating, ventilation & air conditioning (HVAC) systems, lighting systems, and other appliances or systems that significantly contribute to energy use.\n",
            "\n",
            "At the core of this invention is a sophisticated Machine Learning (ML) algorithm, which uses the continuous flow of real-time sensor data to predict and optimize energy usage. This ML model is designed to learn from the data it processes over time, improving its predictive accuracy consistently. The learning process allows the system to understand energy usage patterns better and provide increasingly efficient energy management solutions.\n",
            "\n",
            "In addition to energy efficiency, the system aims to maintain optimal comfort, offering a balance between reducing energy waste and ensuring a comfortable environment for the building’s occupants. Even while reducing energy consumption, it ensures that the standards of comfort, such as optimal temperature and illumination, are uncompromised.\n",
            "\n",
            "Overall, the system significantly contributes to environmental sustainability by optimizing the use of energy resources in smart buildings while enhancing occupants' comfort. It bridges the gap between technology, comfort, and sustainability, making it a cutting-edge solution in improving energy efficiency in smart buildings.\n",
            "\n",
            "    3. Claims:\n",
            "    1. A system for enhancing energy efficiency in smart buildings, characterized by the integration of AI-driven sensors and machine learning algorithms.\n",
            "\n",
            "2. The system of claim 1, wherein said system utilizes real-time data generated from a plurality of sensors to dynamically adjust energy consumption.\n",
            "\n",
            "3. The system of claim 2, wherein said sensors include temperature sensors, lighting sensors, and occupancy sensors.\n",
            "\n",
            "4. The system of claim 2, wherein the dynamic adjustment of energy usage results in decreasing waste while maintaining optimal comfort levels within the smart building.\n",
            "\n",
            "5. The system of claim 1, wherein the machine learning algorithm continuously learns from the data gathered from said sensors.\n",
            "\n",
            "6. The system of claim 5, wherein said learning leads to the improvement of the system's predictive capability and optimization of energy savings.\n",
            "\n",
            "7. The system of claim 1, wherein the AI-driven sensors and machine learning algorithms work in synchronization to respond to changes in temperature, lighting, and occupancy in real-time.\n",
            "\n",
            "8. The system of claim 1, where the AI-driven sensors allow for efficient and accurate data collection on various environmental conditions within the smart building.\n",
            "\n",
            "9. The system of claim 5, wherein the continuous learning by the machine learning algorithm allows for a self-improving system.\n",
            "\n",
            "10. A method for enhancing energy efficiency in smart buildings, characterized by a continuous analysis performed by a system which deploys a combination of AI-driven sensors and machine learning algorithms. \n",
            "\n",
            "11. The method of claim 10, wherein said analysis entails real-time data from temperature, lighting, and occupancy sensors to dynamically adjust energy usage.\n",
            "\n",
            "12. The method of claim 10, where through constant learning from the data, the machine learning algorithm improves its predictions, thereby optimizing energy savings.\n",
            "\n",
            "13. The method of claim 10, which decreases waste while maintaining comfort, by dynamically adjusting energy usage based on the real-time data analysis. \n",
            "\n",
            "14. The method of claim 10, which involves providing ongoing, real-time response and adjustment to changes in temperature, lighting, and occupancy, thus maximizing the efficiency and savings in the energy usage.\n",
            "\n",
            "    \n",
            "Function 'patent_draft_pipeline' took 54.0374 seconds\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\n    Patent Application Draft:\\n\\n    1. Abstract:\\n    The invention pertains to a system for enhancing energy efficiency in smart buildings, employing a blend of AI-driven sensors and machine learning algorithms. The system capitalizes on real-time data gathered from temperature, lighting and occupancy sensors to dynamically adjust energy consumption, mitigating waste while sustaining comfort levels. The system's machine learning model perpetually learns from the accumulated data, enhancing its predictive abilities, and optimizing energy savings, thereby providing a technologically advanced solution for energy conservation and waste reduction in smart buildings.\\n\\n\\n    2. Detailed Description:\\n    Title: Energy Efficiency Improvement System using AI-Driven Sensors and Machine Learning Algorithms in Smart Buildings\\n\\nDetailed Description:\\n\\nThe present invention pertains to an energy management system that leverages Artificial Intelligence (AI) and Machine Learning (ML) capabilities to maximize the energy efficiency of smart buildings. \\n\\nThe system primarily comprises a network of AI-controlled sensors that constantly monitor varying aspects of the building's environmental conditions. These sensors are specifically designed to track temperature fluctuations, lighting conditions, and occupancy levels within the building. The gathered data is real-time in nature, enabling the system to adjust the building's energy consumption instantaneously and dynamically according to changing conditions.\\n\\nA central processing unit, which could be in the form of a dedicated server or cloud-based platform, receives the real-time data from multiple sensors. The data received are processed, and its patterns are analyzed to extrapolate context-specific adjustments in the energy consumption of the building. Potential energy changes may apply but are not limited to, the building’s heating, ventilation & air conditioning (HVAC) systems, lighting systems, and other appliances or systems that significantly contribute to energy use.\\n\\nAt the core of this invention is a sophisticated Machine Learning (ML) algorithm, which uses the continuous flow of real-time sensor data to predict and optimize energy usage. This ML model is designed to learn from the data it processes over time, improving its predictive accuracy consistently. The learning process allows the system to understand energy usage patterns better and provide increasingly efficient energy management solutions.\\n\\nIn addition to energy efficiency, the system aims to maintain optimal comfort, offering a balance between reducing energy waste and ensuring a comfortable environment for the building’s occupants. Even while reducing energy consumption, it ensures that the standards of comfort, such as optimal temperature and illumination, are uncompromised.\\n\\nOverall, the system significantly contributes to environmental sustainability by optimizing the use of energy resources in smart buildings while enhancing occupants' comfort. It bridges the gap between technology, comfort, and sustainability, making it a cutting-edge solution in improving energy efficiency in smart buildings.\\n\\n    3. Claims:\\n    1. A system for enhancing energy efficiency in smart buildings, characterized by the integration of AI-driven sensors and machine learning algorithms.\\n\\n2. The system of claim 1, wherein said system utilizes real-time data generated from a plurality of sensors to dynamically adjust energy consumption.\\n\\n3. The system of claim 2, wherein said sensors include temperature sensors, lighting sensors, and occupancy sensors.\\n\\n4. The system of claim 2, wherein the dynamic adjustment of energy usage results in decreasing waste while maintaining optimal comfort levels within the smart building.\\n\\n5. The system of claim 1, wherein the machine learning algorithm continuously learns from the data gathered from said sensors.\\n\\n6. The system of claim 5, wherein said learning leads to the improvement of the system's predictive capability and optimization of energy savings.\\n\\n7. The system of claim 1, wherein the AI-driven sensors and machine learning algorithms work in synchronization to respond to changes in temperature, lighting, and occupancy in real-time.\\n\\n8. The system of claim 1, where the AI-driven sensors allow for efficient and accurate data collection on various environmental conditions within the smart building.\\n\\n9. The system of claim 5, wherein the continuous learning by the machine learning algorithm allows for a self-improving system.\\n\\n10. A method for enhancing energy efficiency in smart buildings, characterized by a continuous analysis performed by a system which deploys a combination of AI-driven sensors and machine learning algorithms. \\n\\n11. The method of claim 10, wherein said analysis entails real-time data from temperature, lighting, and occupancy sensors to dynamically adjust energy usage.\\n\\n12. The method of claim 10, where through constant learning from the data, the machine learning algorithm improves its predictions, thereby optimizing energy savings.\\n\\n13. The method of claim 10, which decreases waste while maintaining comfort, by dynamically adjusting energy usage based on the real-time data analysis. \\n\\n14. The method of claim 10, which involves providing ongoing, real-time response and adjustment to changes in temperature, lighting, and occupancy, thus maximizing the efficiency and savings in the energy usage.\\n\\n    \""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FUOrsCfGGwqv"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "py310",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.15"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}